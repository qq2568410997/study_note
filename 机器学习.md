# 第一天
## 1. 机器学习概述
1. 什么是机器学习
```
机器学习就是从数据中自动分析并获得规律(模型)，并利用规律对未知数据进行预测
```
## 2. 数据集的结构
1. 机器学习的数据格式一般为
	+ 文件---csv
	+ 一般不以数据库存储
		1. 性能瓶颈，数据大时读写速度很慢
		2. 数据格式不对，不适合机器学习所需
2. pandas读写数据很快
	+ 基于numpy
		1. numpy底层是用C语言写的
		2. numpy释放了GIL全局锁，所以在pandas中使用多线程操作才是真正意义上的多线程并行执行
		3. 原先的python处理速度慢就是因为GIL导致的，python中的多线程并非是真正意义上的并行，仅仅是一种并发(CPU来回切换线程的执行)
3. 数据集结构
	+ 可用数据集
	+ 结构
		1. 特征值   没有得到结果前的样本所具备的特征
		2. 目标值   最终想要得到的结果
	+ 机器学习中不需要对重复值进行处理，因为在学习过程中，可能会对相同的值在不同的时间段内有不同的理解(因为在不同的时间段内的"智力"是不同的)
4. 数据中对于特征的处理
	+ pandas  一个数据读取非常方便以及基本的处理格式的工具
	+ sklearn 对于特征的处理提供了很强大的接口
		1. 处理字符串类型的特征值
## 3. 数据的特征工程
### 3.1 特征工程是什么
1. 特征工程是将原始数据转换为更好地代表预测模型的潜在问题的特征的过程，从而提高了对未知数据的预测准确性
### 3.2 特征工程的意义
1. 直接影响预测结果
### 3.3 scikit-learn库介绍
1. 安装scilit-learn
```
pip install -i http://mirrors.aliyun.com/pypi/simple/ scikit-le arn --trusted-host=mirrors.aliyun.com
```
### 3.4 数据的特征提取
#### 3.4.1 特征抽取实例演示
1. 特征抽取对文本等数据进行特征值化(特征值化就是将其他数据转换为计算机可以更好理解的数据形式)
#### 3.4.2 sklearn特征抽取API
1. sklearn.feature_extraction
#### 3.4.3 字典特征抽取
1. sklearn.feature_extraction.DictVectorizer(sparse=True....)
	+ DictVectorizer.fit_transform(X)
		1. X： 字典或者包含字典的迭代器
		2. 返回值: 返回sparse矩阵
	+ DictVectorizer.inverse_transform(X)
		1. X: array数组或者sparse矩阵
		2. 返回值: 转换为之前的数据格式
	+ DictVectorizer.get_feature_names()
		1. 返回类别名称
	+ DictVectorizer.transform(X)
		1. 按照原先的标准转换
2. sparse矩阵
	+ 元组+值   表示的是什么位置上的什么值
		1. 这样存储的目的是: 节省内存
		2. (1,2) 100  表示的是1行2列的位置上的值是100
3. 字典数据抽取
	+ 就是将给定数组中的每一个字典的类别都转换为一个特征值(数值不作转换)
#### 3.4.4 文本特征抽取
1. sklearn.feature_extraction.text.CountVectorizer()
	+ CountVectorizer.fit_transform(X)
		1. X： 文本或者包含文本的迭代器
		2. 返回值: 返回sparse矩阵
	+ CountVectorizer.inverse_transform(X)
		1. X: array数组或者sparse矩阵
		2. 返回值: 转换为之前的数据格式
	+ CountVectorizer.get_feature_names()
		1. 返回类别名称
2. 文本数据抽取
	+ 就是将给定文本以空格为分隔符进行切分后的所有单词作为特征值
	+ 不统计单个字母、单个汉字(单个词不具备情感指向)
	+ 汉字在进行特征抽取之前需要使用工具进行分词
		1. 使用jieba切词
		2. 在进行文本特征值化
3. sparse矩阵
	+ 该API返回的是sparse矩阵，可以使用toarray()将其转换为数组形式
4. 文本特征抽取的几种方法
	+ count 取词出现的频率
	+ TF-IDF  朴素贝叶斯
		1. TF  term frequency  词的频率
		2. IDF inverse document frequency  逆文档频率
		3. log(总文档数量/该词出现的文档数量)
			+ log(数值)  输入的数值越小，结果越小
		4. TF*IDF  重要性程度

5. TF-IDF 
	+ 主要思想
		1. 如果某个词或短语在一篇文章中出现的频率高，并且在其他文章出现得少，则认为该词或者该短语具有很好的类别区分能力，适合用来分类
	+ 主要作用
		1. 用以评估一个字词对于一个文件集或一个词料库中的其中一份文件的重要程度
	+ API函数
		1. sklearn.feature_extraction.text.TfidfVectorizer()
		2. TfidfVectorizer(stop_words=None,......)
			+ 返回词的权重矩阵
		3. TfidfVectorizer.fit_transform(X)
			+ X： 文本或者包含文本字符串的可迭代对象
			+ 返回值: sparse矩阵
		4. TfidfVectorizer.inverse_transform(X)
			+ X: array数组或者sparse矩阵
			+ 返回值: 转换为之前的数据格式
		5. TfidfVectorizer.get_feature_names()
			+ 返回类别名称
### 3.5 数据的特征预处理
1. 特征处理是什么
	+ 通过特定的统计方法(数学方法)将数据转换成算法要求的数据形式
2. 不同类型的特征数据
	+ 数值型数据---可能需要处理缺失值，一般缺失值在前面就已经被处理了
		1. 标准缩放
			* 归一化
			* 标准化
	+ 类别型数据
		1. one-hot编码
	+ 时间类型
		1. 时间的切分
3. sklearn特征预处理API
	+ sklearn.preprocessing、
	+ sklearn归一化API
		1. sklearn.prteprocessing.MinMaxScaler
		1. MinMaxScaler(feature_range=(0,1)....)
			* 每个特征都缩放到给定的范围内(默认是0-1)
			* MinMaxScaler.fit_transform(X)
				1. X: numpy array格式的数据[n_samples,n_features]
				2. 返回值: 转换后的形状相同的array
	+ sklearn标准化API
		1. sklearn.prteprocessing.MinMaxScaler
		1. MinMaxScaler(feature_range=(0,1)....)
			* 每个特征都所放到给定的范围内(默认是0-1)
			* MinMaxScaler.fit_transform(X)
				1. X: numpy array格式的数据[n_samples,n_features]
				2. 返回值: 转换后的形状相同的array
4. 归一化
	+ 通过对原始数据进行变换把数据映射到0-1(默认区间是0-1)之间
	+ 公式
		1. _X = (x-min)/(max-min) __X = _X*(mx-mi)+mi
			* __X是最终的结果
			* min为当前x所在列的最小值
			* max为当前x所在列的最大值
			* mx、mi分别为指定区间范围的最大最小值。默认mx=1，mi=0
	+ 什么时候进行归一化处理
		1. 当认为当前特征同等重要时
	+ 目的:
		1. 使得某一个特征对最终的结果不会造成更大的影响
	+ 缺点
		1. 异常点直接会影响最大最小值，所以此方法的鲁棒性较差，只适合传统精确小数据场景(现在这种场景并不多见)
5. 标准化
	+ 通过对原始数据进行变换把数据变换到均值为0，标准差为1的范围内
	+ 异常点对平均值的影响不是很大，尤其是数据量越大，异常点对均值的影响就越小
	+ 公式
		1. _X = (x-mean)/标准差 _X为最终的结果
		2. 作用于每一列，mean为均值，除数为标准差，方差开平方就是标准差
		3. 方差越大，稳定性越差，即越不稳定

6. 缺失值处理方法
	+ 丢失(不推荐)
	+ 填补数据(按列->属性)
		1. sklearn中填补API是Imputer
```
# -*- coding: utf-8 -*-
# __author__ : Ricky
# __createTime__ : 2019/11/21  0:34 
# __fileName__ : 002.py
# __devIDE__ : PyCharm

from sklearn.impute import SimpleImputer
import numpy as np

def im():
    """
    缺失值处理
    :return: None
    """
    # NaN nan
    im = SimpleImputer()
    data = im.fit_transform([
        [1,np.nan],
        [np.nan,3],
        [5,8]
    ])
    print(data)
    return None

if __name__ == '__main__':
    im()


```
### 3.6 数据的降维
1. 这里降维中的维度指的是  特征属性的数量即特征列数