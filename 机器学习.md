# 第一天
## 1. 机器学习概述
1. 什么是机器学习
```
机器学习就是从数据中自动分析并获得规律(模型)，并利用规律对未知数据进行预测
```
## 2. 数据集的结构
1. 机器学习的数据格式一般为
	+ 文件---csv
	+ 一般不以数据库存储
		1. 性能瓶颈，数据大时读写速度很慢
		2. 数据格式不对，不适合机器学习所需
2. pandas读写数据很快
	+ 基于numpy
		1. numpy底层是用C语言写的
		2. numpy释放了GIL全局锁，所以在pandas中使用多线程操作才是真正意义上的多线程并行执行
		3. 原先的python处理速度慢就是因为GIL导致的，python中的多线程并非是真正意义上的并行，仅仅是一种并发(CPU来回切换线程的执行)
3. 数据集结构
	+ 可用数据集
	+ 结构
		1. 特征值   没有得到结果前的样本所具备的特征
		2. 目标值   最终想要得到的结果
	+ 机器学习中不需要对重复值进行处理，因为在学习过程中，可能会对相同的值在不同的时间段内有不同的理解(因为在不同的时间段内的"智力"是不同的)
4. 数据中对于特征的处理
	+ pandas  一个数据读取非常方便以及基本的处理格式的工具
	+ sklearn 对于特征的处理提供了很强大的接口
		1. 处理字符串类型的特征值
## 3. 数据的特征工程
### 3.1 特征工程是什么
1. 特征工程是将原始数据转换为更好地代表预测模型的潜在问题的特征的过程，从而提高了对未知数据的预测准确性
### 3.2 特征工程的意义
1. 直接影响预测结果
### 3.3 scikit-learn库介绍
1. 安装scilit-learn
```
pip install -i http://mirrors.aliyun.com/pypi/simple/ scikit-le arn --trusted-host=mirrors.aliyun.com
```
### 3.4 数据的特征提取
#### 3.4.1 特征抽取实例演示
1. 特征抽取对文本等数据进行特征值化(特征值化就是将其他数据转换为计算机可以更好理解的数据形式)
#### 3.4.2 sklearn特征抽取API
1. sklearn.feature_extraction
#### 3.4.3 字典特征抽取
1. sklearn.feature_extraction.DictVectorizer(sparse=True....)
	+ DictVectorizer.fit_transform(X)
		1. X： 字典或者包含字典的迭代器
		2. 返回值: 返回sparse矩阵
	+ DictVectorizer.inverse_transform(X)
		1. X: array数组或者sparse矩阵
		2. 返回值: 转换为之前的数据格式
	+ DictVectorizer.get_feature_names()
		1. 返回类别名称
	+ DictVectorizer.transform(X)
		1. 按照原先的标准转换
2. sparse矩阵
	+ 元组+值   表示的是什么位置上的什么值
		1. 这样存储的目的是: 节省内存
		2. (1,2) 100  表示的是1行2列的位置上的值是100
3. 字典数据抽取
	+ 就是将给定数组中的每一个字典的类别都转换为一个特征值(数值不作转换)
#### 3.4.4 文本特征抽取
1. sklearn.feature_extraction.text.CountVectorizer()
	+ CountVectorizer.fit_transform(X)
		1. X： 文本或者包含文本的迭代器
		2. 返回值: 返回sparse矩阵
	+ CountVectorizer.inverse_transform(X)
		1. X: array数组或者sparse矩阵
		2. 返回值: 转换为之前的数据格式
	+ CountVectorizer.get_feature_names()
		1. 返回类别名称
2. 文本数据抽取
	+ 就是将给定文本以空格为分隔符进行切分后的所有单词作为特征值
	+ 不统计单个字母、单个汉字(单个词不具备情感指向)
	+ 汉字在进行特征抽取之前需要使用工具进行分词
		1. 使用jieba切词
		2. 在进行文本特征值化
3. sparse矩阵
	+ 该API返回的是sparse矩阵，可以使用toarray()将其转换为数组形式
4. 文本特征抽取的几种方法
	+ count 取词出现的频率
	+ TF-IDF  朴素贝叶斯
		1. TF  term frequency  词的频率
		2. IDF inverse document frequency  逆文档频率
		3. log(总文档数量/该词出现的文档数量)
			+ log(数值)  输入的数值越小，结果越小
		4. TF*IDF  重要性程度

5. TF-IDF 
	+ 主要思想
		1. 如果某个词或短语在一篇文章中出现的频率高，并且在其他文章出现得少，则认为该词或者该短语具有很好的类别区分能力，适合用来分类
	+ 主要作用
		1. 用以评估一个字词对于一个文件集或一个词料库中的其中一份文件的重要程度
	+ API函数
		1. sklearn.feature_extraction.text.TfidfVectorizer()
		2. TfidfVectorizer(stop_words=None,......)
			+ 返回词的权重矩阵
		3. TfidfVectorizer.fit_transform(X)
			+ X： 文本或者包含文本字符串的可迭代对象
			+ 返回值: sparse矩阵
		4. TfidfVectorizer.inverse_transform(X)
			+ X: array数组或者sparse矩阵
			+ 返回值: 转换为之前的数据格式
		5. TfidfVectorizer.get_feature_names()
			+ 返回类别名称
### 3.5 数据的特征预处理
1. 特征处理是什么
	+ 通过特定的统计方法(数学方法)将数据转换成算法要求的数据形式
2. 不同类型的特征数据
	+ 数值型数据---可能需要处理缺失值，一般缺失值在前面就已经被处理了
		1. 标准缩放
			* 归一化
			* 标准化
	+ 类别型数据
		1. one-hot编码
	+ 时间类型
		1. 时间的切分
3. sklearn特征预处理API
	+ sklearn.preprocessing、
	+ sklearn归一化API
		1. sklearn.prteprocessing.MinMaxScaler
		1. MinMaxScaler(feature_range=(0,1)....)
			* 每个特征都缩放到给定的范围内(默认是0-1)
			* MinMaxScaler.fit_transform(X)
				1. X: numpy array格式的数据[n_samples,n_features]
				2. 返回值: 转换后的形状相同的array
	+ sklearn标准化API
		1. sklearn.prteprocessing.MinMaxScaler
		1. MinMaxScaler(feature_range=(0,1)....)
			* 每个特征都所放到给定的范围内(默认是0-1)
			* MinMaxScaler.fit_transform(X)
				1. X: numpy array格式的数据[n_samples,n_features]
				2. 返回值: 转换后的形状相同的array
4. 归一化
	+ 通过对原始数据进行变换把数据映射到0-1(默认区间是0-1)之间
	+ 公式
		1. _X = (x-min)/(max-min) __X = _X*(mx-mi)+mi
			* __X是最终的结果
			* min为当前x所在列的最小值
			* max为当前x所在列的最大值
			* mx、mi分别为指定区间范围的最大最小值。默认mx=1，mi=0
	+ 什么时候进行归一化处理
		1. 当认为当前特征同等重要时
	+ 目的:
		1. 使得某一个特征对最终的结果不会造成更大的影响
	+ 缺点
		1. 异常点直接会影响最大最小值，所以此方法的鲁棒性较差，只适合传统精确小数据场景(现在这种场景并不多见)
5. 标准化
	+ 通过对原始数据进行变换把数据变换到均值为0，标准差为1的范围内
	+ 异常点对平均值的影响不是很大，尤其是数据量越大，异常点对均值的影响就越小
	+ 公式
		1. _X = (x-mean)/标准差 _X为最终的结果
		2. 作用于每一列，mean为均值，除数为标准差，方差开平方就是标准差
		3. 方差越大，稳定性越差，即越不稳定

6. 缺失值处理方法
	+ 丢失(不推荐)
	+ 填补数据(按列->属性)
		1. sklearn中填补API是Imputer
```
# -*- coding: utf-8 -*-
# __author__ : Ricky
# __createTime__ : 2019/11/21  0:34 
# __fileName__ : 002.py
# __devIDE__ : PyCharm

from sklearn.impute import SimpleImputer
import numpy as np

def im():
    """
    缺失值处理
    :return: None
    """
    # NaN nan
    im = SimpleImputer()
    data = im.fit_transform([
        [1,np.nan],
        [np.nan,3],
        [5,8]
    ])
    print(data)
    return None

if __name__ == '__main__':
    im()


```
### 3.6 数据的降维
1. 降维中的维度指的是 特征的列数---> 即减少特征数
2. 特征选择
	+ 原因
		1. 冗余: 部分特征的相关度高，容易消耗计算性能
		2. 噪声: 部分特征对预测结果有影响
	+ 概念
		1. 单纯提取所有特征中的某个或某几个作为训练集特征
		2. 特征在选择前和选择后可以改变值，也可以不改变其值
	+ 主要方法
		1. Filter(过滤式): VarianceThreshold--方差阈值
			+ 描述所有样本的特征情况，比如方差为0表示的是该列特征值完全相同，可以删除
			```
			from sklearn.impute import SimpleImputer
			import numpy as np
			def selectFeature():
				var = VarianceThreshold(threshold=0.8)
				data = var.fit_transform([
					[1,2,3],
					[3,4,5],
					[2,8,9]
				])
				print(data)
			if __name__ == '__main__':
				selectFeature()
			```
		2. Embedded(嵌入式): 正则化、决策树
		3. Wrapper(包裹式)
		4. 神经网络
3. 主要成分分析(例如PCA)
	+ PCA的本质是: 一种分析、简化数据集的技术
	+ 目的: 使数据维数压缩，尽可能降低原数据的维度(复杂度)，损失少量信息
	+ 作用: 可以削减回归分析或者聚类分析中特征的数量
	+ 应用场景
		1. 当特征很多时，可以使用PCA简化数据，减少数据特征，处理后的数据的总体特征相比原先，损失很小
	+ 语法
		1. PCA(n_components=None)
			* n_components为小数 表示保留数据的整体特征占比，一般为90%~95%
			* n_components为整数 表示保留多少个特征(一般用小数)
```
### [数据来源](https://www.kaggle.com/c/instacart-market-basket-analysis/data)

import seaborn as sns
import numpy as np
import pandas as pd
import matplotlib as mpl
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA


#%%

# 读取四张表
prior = pd.read_csv('./data/instacart/order_products__prior.csv')
orders = pd.read_csv('./data/instacart/orders.csv')
products = pd.read_csv('./data/instacart/products.csv')
aisles = pd.read_csv('./data/instacart/aisles.csv')

# 根据关联id合并四张表---生成一张所需表
_mg = pd.merge(prior, products, on=['product_id','product_id'])
_mg = pd.merge(_mg, orders, on=['order_id','order_id'])
mt = pd.merge(_mg, aisles, on=['aisle_id','aisle_id'])

# 根据用户分组，用户-物品类别   交叉表(特殊的分组工具)

cross = pd.crosstab(mt['user_id'], mt['aisle'])

print(cross.head(10))

pca = PCA(n_components=0.9)
data = pca.fit_transform(cross)
print(data.shape)
print(data.head())



```
			
4. 高维数据会出现的问题
	+ 特征之间的相关性



## 05-机器学习概述
### 机器学习算法分类
1. 监督学习(预测)
	+ 分类(离散型数据)
		1. k-近邻算法
		2. 贝叶斯
		3. 决策树与随机森林
		4. 逻辑回归
		5. 神经网络
	+ 回归(连续型数据)
		1. 线性回归
		2. 岭回归
	+ 标注
		1. 隐马尔可夫模型
2. 非监督学习
	+ 聚类  k-means
	
### sklearn 数据集
1. 数据集划分
	+ 训练集  70 75 80   建立模型
	+ 测试集  30 25 20   评估模型
2. sklearn数据集接口介绍
	+ sklearn.datasets     加载获取流行数据集
		1. datasets.load_*()  获取小规模数据集，数据包含在datasets中
		2. datasets.fetch_*(data_home=None)  
			* 获取大规模数据集，需要从网络上下载
			* data_home表示的是数据集下载后保存的目录路径，默认是~/scikit_learn_data/
	+ API返回的数据类型是datasets.base.Bunch(字典格式)
		1. data    特征数据数组，numpy.ndarray二维数组
		2. target  标签数组
		3. DESC    数据描述
		4. feature_names  特征名 (新闻数据、手写数字、回归数据没有)
		5. target_names   标签名
3. sklearn分类数据集
	+ sklearn.datasets.load_iris()   加载并返回鸢尾花数据集
	+ sklearn.datasets.load_digits() 加载并返回数字数据集

4. 数据集进行分割(随机分割)
	+ sklearn.model_selection.train_test_split(*array,**option)
		1. x  数据集的特征值
		2. y  数据集的标签值
		3. test_size  测试集的大小，一般为float
		4. random_state  随机数种子
			* 不同的种子会造成不同的随机采样结果
			* 相同的种子采样结果相同
		5. return 训练集特征值，测试集特征值，训练标签，测试标签(默认随机取)
5. 用于分类的大数据集
	+ sklearn.datasets.fetch_20newsgroups(data_home=None,subset='train')
		1. subset
			* train  训练集
			* test   测试集
			* all    两者的全部
	+ datasets.clear_data_home(data_home=None)
		1. 清除指定目录下的数据集
6. sklearn回归数据集
	+ sklearn.datasets.load_boston()   加载并返回波士顿房价数据集
	+ sklearn.datasets.load_diabets()  加载并返回糖尿病数据集
7. 转换器--(原始数据X--->经过fit_transform(X)转换后的数据集)
	+ 实例化(实例化的是一个转换器类--Transformer)
	+ 调用fit_transform(对于文档建立分词词频矩阵)
		1. fit_transform  ===> fit+transform(输入数据+转换)
		2. fit+transform  会引发的问题
			* 输入A数据，就会在内部计算出属于A数据的一些特征值
			* 如果转换的是B数据，就会将A数据的特征应用到B数据中，
			* 所以A和B数据必须相同
8. 估计器
	+ 在sklearn中，估计器(estimator)是一个重要的角色，是一类实现了算法的API
	+ 用于分类的估计器
		1. sklearn.neighbors  k-近邻算法
		2. sklearn.naive_bayes贝叶斯
		3. sklearn.linear_model.LogisticRegression  逻辑回归
		4. sklearn.tree       决策树与随机森林
	+ 用于回归的估计器
		1. sklearn.linear_model.LinearRegression  线性回归
		2. sklearn.linear_model.Ridge  岭回归
	+ 工作流程
		1. 训练集(x_train,y_train)--特征值x,目标值y
		2. fit(x_train,y_train)
		3. estimator
			+ 结果精度   score(x_test,y_test)
			+ 预测结果   y_predict = predict(x_test)
		4. 测试集(x_test,y_test)


### k-近邻算法(KNN算法)---需要做标准化
1. 通过邻居的特征来判断自己的所属类型
2. 定义: 如果一个样本在特征空间中的k个最相似(即特征空间中最邻近)的样本大多数属于某个类别，那么则可以推断出该样本也属于这个类别
	+ 相近的样本，特征值也是相近的
3. 距离公式: 欧氏距离
	+ 对应特征之差求平方和
	+ 对结果开根号
4. API
	+ sklearn.neighbors.KNeighborsClassifilter(n_neighbors=5,algorithm='auto')
		1. n_neighbors: int,可选 默认是5  表示查询的邻居数
		2. algorithm: {'auto','ball_tree','kd_trree','brute'}可选用计算最近邻居的算法
			* ball_tree 将会使用BallTree
			* kd_tree   将会使用KDTree
			* auto    尝试根据传递给fit方法的值来决定最合适的算法(不同的实现方式影响效率)

5. 案例---预测入住位置
[数据链接](https://www.kaggle.com/c/facebook-v-predicting-check-ins/data)
```
# -*- coding: utf-8 -*-
# __author__ : Ricky
# __createTime__ : 2019/11/24  20:24 
# __fileName__ : FBLocation.py
# __devIDE__ : PyCharm
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler

def knncls():
    """
    K-近邻预测用户签到位置
    :return: None
    """

    # 读取数据
    data = pd.read_csv('./data/FBlocation/train.csv')
    print(data.head(10))

    # 处理数据
    # 首先 缩小数据  样本太大就减少样本量
    # 使用query查询数据筛选
    data = data.query("x>1.0 & x<1.25 & y>2.5 & y<2.75")

    # 对时间错进行处理  转换为日期格式，然后将年月日作为数据的新特征使用
    time_value = pd.to_datetime(data['time'], unit='s')
    print(time_value)
    # 将日期时间转换为 可以操作的字典格式
    time_value = pd.DatetimeIndex(time_value)

    # 构造一些特征
    data['day'] = time_value.day
    data['hour'] = time_value.hour
    data['weekday'] = time_value.weekday

    # 删除之前的时间戳特征  pd中的axis=1表示的是列
    data = data.drop(['time'], axis=1)
    print(data)
    # 把签到数量少于n个目标位置的删除
    # 按照列字段分组 列字段就会称为新数据的索引
    place_count = data.groupby('place_id').count()
    print(place_count)
    # reset_index  让索引称为特征列
    tf = place_count[place_count.row_id>3].reset_index()
    print(tf)
    # 取出符合条件的数据
    data = data[data['place_id'].isin(tf.place_id)]
    print(data)

    ### 预处理结束
    # 取出特征值和目标值
    y = data['place_id']
    x = data.drop(['place_id'], axis=1)

    # 进行数据的分割  训练集 测试集
    x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.25)

    # 特征工程(标准化) 对数据集进行标准化
    # std = StandardScaler()
    # x_train = std.fit_transform(x_train)
    # ## 这里直接使用transform就可以了 再fit一次就需要再次计算平均值和标准差 没必要
    # x_test = std.transform(x_test)
    # 进行k-近邻算法
    knn = KNeighborsClassifier(n_neighbors=5)

    # fit predict score
    knn.fit(x_train,y_train)

    # 得出预测结果
    y_predict = knn.predict(x_test)
    print(y_predict)

    # 得出评估准确性
    print(f'当前模型准确性为: {knn.score(x_test, y_test)}')

    return None


if __name__ == '__main__':
    knncls()

```
#### k-近邻算法总结
1. k值取多大，有什么影响
	+ k值取很小: 比如取1个，容易受异常点的影响
	+ k值取很大: 比如取100个，容易受类别的波动(可能会出现很多种类别)
2. 性能问题
	+ 样本量大，时间复杂度极高
3. 优缺点
	+ 优点
		1. 简单
		2. 易于理解
		3. 易于实现
		4. 无需估计参数  算法实例化时的参数是超参数，可调节的
		5. 无需训练     不需要多次迭代训练，数据一定，计算一次即可算出距离，迭代无法改变准确性
	+ 缺点
		1. 懒惰算法，对测试样本分类时的计算量大，内存开销大
		2. 必须指定k值，k值选择不当则分类精度不能保证
4. 使用场景
	+ 小数据场景，几千~几万样本
	+ 具体场景具体业务去测试
5. 在实例化算法时，可以选用不同的数据结构以加快速度
```
knn = KNeighborsClassifier(n_neighbors=5,algorithm='kd_tree')
```
6. k-近邻算法作业---对鸢尾花进行分类

### 朴素贝叶斯算法----不需要进行标准化
1. 概率基础
	+ 定义: 一件事发生的可能性
	+ 联合概率: 包含多个条件，且所有条件同时成立的概率
		1. 记作: P(A,B)
		2. P(A,B) = P(A)P(B)
	+ 条件概率: 就是事件A在另一个事件B已经发生的条件下发生的概率
		1. 记作: P(A|B)
		2. P(A,C|B) = P(A|B)P(C|B)  注意:这里的A和C是相互独立的
2. 朴素贝叶斯
	+ 朴素表示的就是条件独立(指的就是特征之间相互独立)
	+ 贝叶斯公式
	```
	P(C|W) = (P(W|C)P(C))/P(W)
	### 解析
		+ w为给定文档的特征值
		+ c为文档类别
	```
		1. P(C): 每个文档类别的概率(某文档类别数/总文档数量)
		2. P(W|C): 给定类别下特征(被预测文档中出现的词)的概率
			* 计算方法: P(F1|C) = Ni/N
				1. Ni  为该F1词在C类别所有文档中出现的次数
				2. N   为所属类别C下的文档所有词出现的次数之和
			* 拉普拉斯平滑系数(解决P(F1|C) = 0 的问题)
				1. 计算方法: P(F1|C) = (Ni+a)/(N+am)
					* a为指定的系数一般为1
					* m为训练文档中统计出的特征词个数
		3. P(F1,F2..): 预测文档中每个词的概率
3. sklearn朴素贝叶斯实现API
	+ sklearn.naive_bayes.MultinomiaINB(alpha=1)
		1. alpha 拉普拉斯平滑系数---用来解决某词概率为0的问题
4. 朴素贝叶斯分类的优缺点
	+ 优点
		1. 朴素贝叶斯模型发源于古典数学模型，有稳定的分类效率
		2. 对缺失数据不太敏感，算法比较简单，常用于文本分类
		3. 分类准确性高，速度快
	+ 缺点
		1. 由于使用了样本属性独立性的假设，所以如果样本属性有关联，效果就不是很好
5. 分类模型的评估
	+ estimator.score()  使用准确率，即预测结果正确的百分比
	+ Precision          使用精确率，即预测结果为正例的样本中，真实结果为正例的比例(查的准)
	+ Recall             使用召回率，即真实结果为正例的样本中，测试结果为正例的比例(查的全，对正样本的区分能力)
	+ 混淆矩阵
		1.                    预测结果
		2. 真实结果              正例        假例
			+ 正例              真正例TP     伪反例FN
			+ 假例              伪正例FP     真反例TN
			+ T  True   
			+ F  False
			+ P  Positive
			+ N  Negative
	+ F1-score            反映模型的稳健型,值越大表示越稳健
	```
	F1 = 2TP/(2TP+FN+FP) 化简得  F1 = 2Precision*Recall/(Precision+Recall)
	```
	+ API
		1. sklearn.metrics.classification_report(y_true,y_pred,target_names)
		```
		print('当前评估报告为: ', classification_report(y_test,y_predict, target_names=news.target_names))
		```
		
#### 交叉验证
1. 定义: 将拿到的训练集数据，分为训练和验证集，将整体数据n等分，依次用其中的一份作为验证集
	+ 比如5等分，依次使用其中一份数据为验证集，就会得到5组模型，分别对5组模型求结果再取均值就是最终的验证值
	+ 又称为5折交叉验证
#### 超参数搜索----网格搜索(调参数)
1. 在通常情况下，有很多参数需要手动指定(如k-近邻苏算法中的K值)，这种叫做超参数
	+ 手动过程繁杂，所以需要对模型预设几种超参数组合
		1. 超参数不止一个时，需要使用排列组合，看一共有几种组合形式
	+ 每组超参数都采用交叉验证来进行评估
		1. 10折交叉验证
	+ 最终选择最优参数组合来建立模型
2. API
	+ sklearn.model_selection.GridSearchCV(estimator,param_grid=None,cv=None)
		1. CV 表示的是cross validation 交叉验证
		2. estimator 估计器对象
		3. param_grid 估计器参数  dict-->{'n_neighbors':[1,2,5]}
		4. cv  指定的几折交叉验证
		5. fit  输入数据
		6. score 准确率
		7. 结果分析
			+ best_score_: 在交叉验证中验证的最好结果
			+ best_estimator_: 最好的参数模型
			+ cv_results_: 每次交叉验证后的验证集和训练集的准确率结果

```
# -*- coding: utf-8 -*-
# __author__ : Ricky
# __createTime__ : 2019/11/24  20:24 
# __fileName__ : FBLocation.py
# __devIDE__ : PyCharm
import pandas as pd
from sklearn.model_selection import train_test_split,GridSearchCV
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler


def knncls():
    """
    K-近邻预测用户签到位置
    :return: None
    """

    # 读取数据
    data = pd.read_csv('./data/FBlocation/train.csv')
    print(data.head(10))

    # 处理数据
    # 首先 缩小数据  样本太大就减少样本量
    # 使用query查询数据筛选
    data = data.query("x>1.0 & x<1.25 & y>2.5 & y<2.75")

    # 对时间错进行处理  转换为日期格式，然后将年月日作为数据的新特征使用
    time_value = pd.to_datetime(data['time'], unit='s')
    print(time_value)
    # 将日期时间转换为 可以操作的字典格式
    time_value = pd.DatetimeIndex(time_value)

    # 构造一些特征
    data['day'] = time_value.day
    data['hour'] = time_value.hour
    data['weekday'] = time_value.weekday

    # 删除之前的时间戳特征  pd中的axis=1表示的是列
    data = data.drop(['time'], axis=1)
    print(data)
    # 把签到数量少于n个目标位置的删除
    # 按照列字段分组 列字段就会称为新数据的索引
    place_count = data.groupby('place_id').count()
    print(place_count)
    # reset_index  让索引称为特征列
    tf = place_count[place_count.row_id>3].reset_index()
    print(tf)
    # 取出符合条件的数据
    data = data[data['place_id'].isin(tf.place_id)]
    print(data)

    ### 预处理结束
    # 取出特征值和目标值
    y = data['place_id']
    x = data.drop(['place_id'], axis=1)

    # 进行数据的分割  训练集 测试集
    x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.25)

    # 特征工程(标准化) 对数据集进行标准化
    # std = StandardScaler()
    # x_train = std.fit_transform(x_train)
    # ## 这里直接使用transform就可以了 再fit一次就需要再次计算平均值和标准差 没必要
    # x_test = std.transform(x_test)
    # 进行k-近邻算法
    knn = KNeighborsClassifier()

    # # fit predict score
    # knn.fit(x_train,y_train)
    #
    # # 得出预测结果
    # y_predict = knn.predict(x_test)
    # print(y_predict)
    #
    # # 得出评估准确性
    # print(f'当前模型准确性为: {knn.score(x_test, y_test)}')
    params = {'n_neighbors': [3,5,7,10]}
    # fit predict score
    gcv = GridSearchCV(knn,param_grid=params, cv=5)
    gcv.fit(x_train, y_train)

    # 得出评估准确性
    print(f'当前模型准确性为: {gcv.score(x_test, y_test)}')

    # 在交叉验证中验证的最好结果
    print(f'当前交叉验证中验证的最好结果是: {gcv.best_score_}')
    print(f'当前交叉验证中验证的最好的参数模型是: {gcv.best_estimator_}')
    print(f'每次超参数组合交叉验证的结果是:: {gcv.cv_results_}')

    return None


if __name__ == '__main__':
    knncls()

```

### 分类算法----决策树、随机森林
1. 信息熵
	+ H(X) = P(x)logP(x)  (x表示的是在X特征下的各个类别)
	+ 单位是比特
	+ 在一无所知的情况下，信息熵最大，即付出的代价最大
		1. 举例: 32支球队夺冠的几率相同时，对应的信息熵等于5比特  log32=5(此时的信息熵也是最大的)
2. 信息和消除不确定性是相联系的
	+ 信息熵越大，表示不确定越大，预测付出的代价就越大
	+ 信息熵越小，表示已知性越大，预测付出的代价就越小

3. 决策树的分类依据之一
	+ 信息增益
		1. 当得知一个特征条件之后，减少的信息熵的大小
	+ 公式
		1. g(D|A) = H(D) - H(D|A)   
			+ A表示的是特征列
			+ 举例
				* g(D|年龄) = H(D) - H(D'|年龄)
					= H(D) - [1/3H(青年)+1/3H(中年)+1/3H(青老年]
					
4. 常见的决策树使用的算法
	+ ID3  信息增益  最大的准则
	+ C4.5 信息增益比  最大的准则
	+ CART 
		1. 回归树:  平方误差 最小
		2. 分类树:  基尼系数  最小的准则  在sklearn中可以选择划分的原则