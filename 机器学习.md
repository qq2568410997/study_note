# 第一天
## 1. 机器学习概述
1. 什么是机器学习
```
机器学习就是从数据中自动分析并获得规律(模型)，并利用规律对未知数据进行预测
```
## 2. 数据集的结构
1. 机器学习的数据格式一般为
	+ 文件---csv
	+ 一般不以数据库存储
		1. 性能瓶颈，数据大时读写速度很慢
		2. 数据格式不对，不适合机器学习所需
2. pandas读写数据很快
	+ 基于numpy
		1. numpy底层是用C语言写的
		2. numpy释放了GIL全局锁，所以在pandas中使用多线程操作才是真正意义上的多线程并行执行
		3. 原先的python处理速度慢就是因为GIL导致的，python中的多线程并非是真正意义上的并行，仅仅是一种并发(CPU来回切换线程的执行)
3. 数据集结构
	+ 可用数据集
	+ 结构
		1. 特征值   没有得到结果前的样本所具备的特征
		2. 目标值   最终想要得到的结果
	+ 机器学习中不需要对重复值进行处理，因为在学习过程中，可能会对相同的值在不同的时间段内有不同的理解(因为在不同的时间段内的"智力"是不同的)
4. 数据中对于特征的处理
	+ pandas  一个数据读取非常方便以及基本的处理格式的工具
	+ sklearn 对于特征的处理提供了很强大的接口
		1. 处理字符串类型的特征值
## 3. 数据的特征工程
### 3.1 特征工程是什么
1. 特征工程是将原始数据转换为更好地代表预测模型的潜在问题的特征的过程，从而提高了对未知数据的预测准确性
### 3.2 特征工程的意义
1. 直接影响预测结果
### 3.3 scikit-learn库介绍
1. 安装scilit-learn
```
pip install -i http://mirrors.aliyun.com/pypi/simple/ scikit-le arn --trusted-host=mirrors.aliyun.com
```
### 3.4 数据的特征提取
#### 3.4.1 特征抽取实例演示
1. 特征抽取对文本等数据进行特征值化(特征值化就是将其他数据转换为计算机可以更好理解的数据形式)
#### 3.4.2 sklearn特征抽取API
1. sklearn.feature_extraction
#### 3.4.3 字典特征抽取
1. sklearn.feature_extraction.DictVectorizer(sparse=True....)
	+ DictVectorizer.fit_transform(X)
		1. X： 字典或者包含字典的迭代器
		2. 返回值: 返回sparse矩阵
	+ DictVectorizer.inverse_transform(X)
		1. X: array数组或者sparse矩阵
		2. 返回值: 转换为之前的数据格式
	+ DictVectorizer.get_feature_names()
		1. 返回类别名称
	+ DictVectorizer.transform(X)
		1. 按照原先的标准转换
2. sparse矩阵
	+ 元组+值   表示的是什么位置上的什么值
		1. 这样存储的目的是: 节省内存
		2. (1,2) 100  表示的是1行2列的位置上的值是100
3. 字典数据抽取
	+ 就是将给定数组中的每一个字典的类别都转换为一个特征值(数值不作转换)
	+ 用one-hot编码来表示每一个特征
#### 3.4.4 文本特征抽取
1. sklearn.feature_extraction.text.CountVectorizer()
	+ CountVectorizer.fit_transform(X)
		1. X： 文本或者包含文本的迭代器
		2. 返回值: 返回sparse矩阵
	+ CountVectorizer.inverse_transform(X)
		1. X: array数组或者sparse矩阵
		2. 返回值: 转换为之前的数据格式
	+ CountVectorizer.get_feature_names()
		1. 返回类别名称
2. 文本数据抽取
	+ 就是将给定文本以空格为分隔符进行切分后的所有单词作为特征值
	+ 不统计单个字母、单个汉字(单个词不具备情感指向)
	+ 汉字在进行特征抽取之前需要使用工具进行分词
		1. 使用jieba切词
		2. 在进行文本特征值化
3. sparse矩阵
	+ 该API返回的是sparse矩阵，可以使用toarray()将其转换为数组形式
4. 文本特征抽取的几种方法
	+ count 取词出现的频率
	+ TF-IDF  朴素贝叶斯
		1. TF  term frequency  词的频率
		2. IDF inverse document frequency  逆文档频率
		3. log(总文档数量/该词出现的文档数量)
			+ log(数值)  输入的数值越小，结果越小
		4. TF*IDF  重要性程度

5. TF-IDF 
	+ 主要思想
		1. 如果某个词或短语在一篇文章中出现的频率高，并且在其他文章出现得少，则认为该词或者该短语具有很好的类别区分能力，适合用来分类
	+ 主要作用
		1. 用以评估一个字词对于一个文件集或一个词料库中的其中一份文件的重要程度
	+ API函数
		1. sklearn.feature_extraction.text.TfidfVectorizer()
		2. TfidfVectorizer(stop_words=None,......)
			+ 返回词的权重矩阵
		3. TfidfVectorizer.fit_transform(X)
			+ X： 文本或者包含文本字符串的可迭代对象
			+ 返回值: sparse矩阵
		4. TfidfVectorizer.inverse_transform(X)
			+ X: array数组或者sparse矩阵
			+ 返回值: 转换为之前的数据格式
		5. TfidfVectorizer.get_feature_names()
			+ 返回类别名称
### 3.5 数据的特征预处理
1. 特征处理是什么
	+ 通过特定的统计方法(数学方法)将数据转换成算法要求的数据形式
2. 不同类型的特征数据
	+ 数值型数据---可能需要处理缺失值，一般缺失值在前面就已经被处理了
		1. 标准缩放
			* 归一化
			* 标准化
	+ 类别型数据
		1. one-hot编码
	+ 时间类型
		1. 时间的切分
3. sklearn特征预处理API
	+ sklearn.preprocessing、
	+ sklearn归一化API
		1. sklearn.prteprocessing.MinMaxScaler
		1. MinMaxScaler(feature_range=(0,1)....)
			* 每个特征都缩放到给定的范围内(默认是0-1)
			* MinMaxScaler.fit_transform(X)
				1. X: numpy array格式的数据[n_samples,n_features]
				2. 返回值: 转换后的形状相同的array
	+ sklearn标准化API
		1. sklearn.prteprocessing.MinMaxScaler
		1. MinMaxScaler(feature_range=(0,1)....)
			* 每个特征都所放到给定的范围内(默认是0-1)
			* MinMaxScaler.fit_transform(X)
				1. X: numpy array格式的数据[n_samples,n_features]
				2. 返回值: 转换后的形状相同的array
4. 归一化
	+ 通过对原始数据进行变换把数据映射到0-1(默认区间是0-1)之间
	+ 公式
		1. _X = (x-min)/(max-min) __X = _X*(mx-mi)+mi
			* __X是最终的结果
			* min为当前x所在列的最小值
			* max为当前x所在列的最大值
			* mx、mi分别为指定区间范围的最大最小值。默认mx=1，mi=0
	+ 什么时候进行归一化处理
		1. 当认为当前特征同等重要时
	+ 目的:
		1. 使得某一个特征对最终的结果不会造成更大的影响
	+ 缺点
		1. 异常点直接会影响最大最小值，所以此方法的鲁棒性较差，只适合传统精确小数据场景(现在这种场景并不多见)
5. 标准化
	+ 通过对原始数据进行变换把数据变换到均值为0，标准差为1的范围内
	+ 异常点对平均值的影响不是很大，尤其是数据量越大，异常点对均值的影响就越小
	+ 公式
		1. _X = (x-mean)/标准差 _X为最终的结果
		2. 作用于每一列，mean为均值，除数为标准差，方差开平方就是标准差
		3. 方差越大，稳定性越差，即越不稳定

6. 缺失值处理方法
	+ 丢失(不推荐)
	+ 填补数据(按列->属性)
		1. sklearn中填补API是Imputer
```
# -*- coding: utf-8 -*-
# __author__ : Ricky
# __createTime__ : 2019/11/21  0:34 
# __fileName__ : 002.py
# __devIDE__ : PyCharm

from sklearn.impute import SimpleImputer
import numpy as np

def im():
    """
    缺失值处理
    :return: None
    """
    # NaN nan
    im = SimpleImputer()
    data = im.fit_transform([
        [1,np.nan],
        [np.nan,3],
        [5,8]
    ])
    print(data)
    return None

if __name__ == '__main__':
    im()


```
### 3.6 数据的降维
1. 降维中的维度指的是 特征的列数---> 即减少特征数
2. 特征选择
	+ 原因
		1. 冗余: 部分特征的相关度高，容易消耗计算性能
		2. 噪声: 部分特征对预测结果有影响
	+ 概念
		1. 单纯提取所有特征中的某个或某几个作为训练集特征
		2. 特征在选择前和选择后可以改变值，也可以不改变其值
	+ 主要方法
		1. Filter(过滤式): VarianceThreshold--方差阈值
			+ 描述所有样本的特征情况，比如方差为0表示的是该列特征值完全相同，可以删除
			```
			from sklearn.impute import SimpleImputer
			import numpy as np
			def selectFeature():
				var = VarianceThreshold(threshold=0.8)
				data = var.fit_transform([
					[1,2,3],
					[3,4,5],
					[2,8,9]
				])
				print(data)
			if __name__ == '__main__':
				selectFeature()
			```
		2. Embedded(嵌入式): 正则化、决策树
		3. Wrapper(包裹式)
		4. 神经网络
3. 主要成分分析(例如PCA)
	+ PCA的本质是: 一种分析、简化数据集的技术
	+ 目的: 使数据维数压缩，尽可能降低原数据的维度(复杂度)，损失少量信息
	+ 作用: 可以削减回归分析或者聚类分析中特征的数量
	+ 应用场景
		1. 当特征很多时，可以使用PCA简化数据，减少数据特征，处理后的数据的总体特征相比原先，损失很小
	+ 语法
		1. PCA(n_components=None)
			* n_components为小数 表示保留数据的整体特征占比，一般为90%~95%
			* n_components为整数 表示保留多少个特征(一般用小数)
```
### [数据来源](https://www.kaggle.com/c/instacart-market-basket-analysis/data)

import seaborn as sns
import numpy as np
import pandas as pd
import matplotlib as mpl
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA


#%%

# 读取四张表
prior = pd.read_csv('./data/instacart/order_products__prior.csv')
orders = pd.read_csv('./data/instacart/orders.csv')
products = pd.read_csv('./data/instacart/products.csv')
aisles = pd.read_csv('./data/instacart/aisles.csv')

# 根据关联id合并四张表---生成一张所需表
_mg = pd.merge(prior, products, on=['product_id','product_id'])
_mg = pd.merge(_mg, orders, on=['order_id','order_id'])
mt = pd.merge(_mg, aisles, on=['aisle_id','aisle_id'])

# 根据用户分组，用户-物品类别   交叉表(特殊的分组工具)

cross = pd.crosstab(mt['user_id'], mt['aisle'])

print(cross.head(10))

pca = PCA(n_components=0.9)
data = pca.fit_transform(cross)
print(data.shape)
print(data.head())



```
			
4. 高维数据会出现的问题
	+ 特征之间的相关性



## 05-机器学习概述
### 机器学习算法分类
1. 监督学习(预测)
	+ 分类(离散型数据)
		1. k-近邻算法
		2. 贝叶斯
		3. 决策树与随机森林
		4. 逻辑回归
		5. 神经网络
	+ 回归(连续型数据)
		1. 线性回归
		2. 岭回归
	+ 标注
		1. 隐马尔可夫模型
2. 非监督学习
	+ 聚类  k-means
	
### sklearn 数据集
1. 数据集划分
	+ 训练集  70 75 80   建立模型
	+ 测试集  30 25 20   评估模型
2. sklearn数据集接口介绍
	+ sklearn.datasets     加载获取流行数据集
		1. datasets.load_*()  获取小规模数据集，数据包含在datasets中
		2. datasets.fetch_*(data_home=None)  
			* 获取大规模数据集，需要从网络上下载
			* data_home表示的是数据集下载后保存的目录路径，默认是~/scikit_learn_data/
	+ API返回的数据类型是datasets.base.Bunch(字典格式)
		1. data    特征数据数组，numpy.ndarray二维数组
		2. target  标签数组
		3. DESC    数据描述
		4. feature_names  特征名 (新闻数据、手写数字、回归数据没有)
		5. target_names   标签名
3. sklearn分类数据集
	+ sklearn.datasets.load_iris()   加载并返回鸢尾花数据集
	+ sklearn.datasets.load_digits() 加载并返回数字数据集

4. 数据集进行分割(随机分割)
	+ sklearn.model_selection.train_test_split(*array,**option)
		1. x  数据集的特征值
		2. y  数据集的标签值
		3. test_size  测试集的大小，一般为float
		4. random_state  随机数种子
			* 不同的种子会造成不同的随机采样结果
			* 相同的种子采样结果相同
		5. return 训练集特征值，测试集特征值，训练标签，测试标签(默认随机取)
5. 用于分类的大数据集
	+ sklearn.datasets.fetch_20newsgroups(data_home=None,subset='train')
		1. subset
			* train  训练集
			* test   测试集
			* all    两者的全部
	+ datasets.clear_data_home(data_home=None)
		1. 清除指定目录下的数据集
6. sklearn回归数据集
	+ sklearn.datasets.load_boston()   加载并返回波士顿房价数据集
	+ sklearn.datasets.load_diabets()  加载并返回糖尿病数据集
7. 转换器--(原始数据X--->经过fit_transform(X)转换后的数据集)
	+ 实例化(实例化的是一个转换器类--Transformer)
	+ 调用fit_transform(对于文档建立分词词频矩阵)
		1. fit_transform  ===> fit+transform(输入数据+转换)
		2. fit+transform  会引发的问题
			* 输入A数据，就会在内部计算出属于A数据的一些特征值
			* 如果转换的是B数据，就会将A数据的特征应用到B数据中，
			* 所以A和B数据必须相同
8. 估计器
	+ 在sklearn中，估计器(estimator)是一个重要的角色，是一类实现了算法的API
	+ 用于分类的估计器
		1. sklearn.neighbors  k-近邻算法
		2. sklearn.naive_bayes贝叶斯
		3. sklearn.linear_model.LogisticRegression  逻辑回归
		4. sklearn.tree       决策树与随机森林
	+ 用于回归的估计器
		1. sklearn.linear_model.LinearRegression  线性回归
		2. sklearn.linear_model.Ridge  岭回归
	+ 工作流程
		1. 训练集(x_train,y_train)--特征值x,目标值y
		2. fit(x_train,y_train)
		3. estimator
			+ 结果精度   score(x_test,y_test)
			+ 预测结果   y_predict = predict(x_test)
		4. 测试集(x_test,y_test)


### k-近邻算法(KNN算法)---需要做标准化
1. 通过邻居的特征来判断自己的所属类型
2. 定义: 如果一个样本在特征空间中的k个最相似(即特征空间中最邻近)的样本大多数属于某个类别，那么则可以推断出该样本也属于这个类别
	+ 相近的样本，特征值也是相近的
3. 距离公式: 欧氏距离
	+ 对应特征之差求平方和
	+ 对结果开根号
4. API
	+ sklearn.neighbors.KNeighborsClassifilter(n_neighbors=5,algorithm='auto')
		1. n_neighbors: int,可选 默认是5  表示查询的邻居数
		2. algorithm: {'auto','ball_tree','kd_trree','brute'}可选用计算最近邻居的算法
			* ball_tree 将会使用BallTree
			* kd_tree   将会使用KDTree
			* auto    尝试根据传递给fit方法的值来决定最合适的算法(不同的实现方式影响效率)

5. 案例---预测入住位置
[数据链接](https://www.kaggle.com/c/facebook-v-predicting-check-ins/data)
```
# -*- coding: utf-8 -*-
# __author__ : Ricky
# __createTime__ : 2019/11/24  20:24 
# __fileName__ : FBLocation.py
# __devIDE__ : PyCharm
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler

def knncls():
    """
    K-近邻预测用户签到位置
    :return: None
    """

    # 读取数据
    data = pd.read_csv('./data/FBlocation/train.csv')
    print(data.head(10))

    # 处理数据
    # 首先 缩小数据  样本太大就减少样本量
    # 使用query查询数据筛选
    data = data.query("x>1.0 & x<1.25 & y>2.5 & y<2.75")

    # 对时间错进行处理  转换为日期格式，然后将年月日作为数据的新特征使用
    time_value = pd.to_datetime(data['time'], unit='s')
    print(time_value)
    # 将日期时间转换为 可以操作的字典格式
    time_value = pd.DatetimeIndex(time_value)

    # 构造一些特征
    data['day'] = time_value.day
    data['hour'] = time_value.hour
    data['weekday'] = time_value.weekday

    # 删除之前的时间戳特征  pd中的axis=1表示的是列
    data = data.drop(['time'], axis=1)
    print(data)
    # 把签到数量少于n个目标位置的删除
    # 按照列字段分组 列字段就会称为新数据的索引
    place_count = data.groupby('place_id').count()
    print(place_count)
    # reset_index  让索引称为特征列
    tf = place_count[place_count.row_id>3].reset_index()
    print(tf)
    # 取出符合条件的数据
    data = data[data['place_id'].isin(tf.place_id)]
    print(data)

    ### 预处理结束
    # 取出特征值和目标值
    y = data['place_id']
    x = data.drop(['place_id'], axis=1)

    # 进行数据的分割  训练集 测试集
    x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.25)

    # 特征工程(标准化) 对数据集进行标准化
    # std = StandardScaler()
    # x_train = std.fit_transform(x_train)
    # ## 这里直接使用transform就可以了 再fit一次就需要再次计算平均值和标准差 没必要
    # x_test = std.transform(x_test)
    # 进行k-近邻算法
    knn = KNeighborsClassifier(n_neighbors=5)

    # fit predict score
    knn.fit(x_train,y_train)

    # 得出预测结果
    y_predict = knn.predict(x_test)
    print(y_predict)

    # 得出评估准确性
    print(f'当前模型准确性为: {knn.score(x_test, y_test)}')

    return None


if __name__ == '__main__':
    knncls()

```
#### k-近邻算法总结
1. k值取多大，有什么影响
	+ k值取很小: 比如取1个，容易受异常点的影响
	+ k值取很大: 比如取100个，容易受类别的波动(可能会出现很多种类别)
2. 性能问题
	+ 样本量大，时间复杂度极高
3. 优缺点
	+ 优点
		1. 简单
		2. 易于理解
		3. 易于实现
		4. 无需估计参数  算法实例化时的参数是超参数，可调节的
		5. 无需训练     不需要多次迭代训练，数据一定，计算一次即可算出距离，迭代无法改变准确性
	+ 缺点
		1. 懒惰算法，对测试样本分类时的计算量大，内存开销大
		2. 必须指定k值，k值选择不当则分类精度不能保证
4. 使用场景
	+ 小数据场景，几千~几万样本
	+ 具体场景具体业务去测试
5. 在实例化算法时，可以选用不同的数据结构以加快速度
```
knn = KNeighborsClassifier(n_neighbors=5,algorithm='kd_tree')
```
6. k-近邻算法作业---对鸢尾花进行分类

### 朴素贝叶斯算法----不需要进行标准化
1. 概率基础
	+ 定义: 一件事发生的可能性
	+ 联合概率: 包含多个条件，且所有条件同时成立的概率
		1. 记作: P(A,B)
		2. P(A,B) = P(A)P(B)
	+ 条件概率: 就是事件A在另一个事件B已经发生的条件下发生的概率
		1. 记作: P(A|B)
		2. P(A,C|B) = P(A|B)P(C|B)  注意:这里的A和C是相互独立的
2. 朴素贝叶斯
	+ 朴素表示的就是条件独立(指的就是特征之间相互独立)
	+ 贝叶斯公式
	```
	P(C|W) = (P(W|C)P(C))/P(W)
	### 解析
		+ w为给定文档的特征值
		+ c为文档类别
	```
		1. P(C): 每个文档类别的概率(某文档类别数/总文档数量)
		2. P(W|C): 给定类别下特征(被预测文档中出现的词)的概率
			* 计算方法: P(F1|C) = Ni/N
				1. Ni  为该F1词在C类别所有文档中出现的次数
				2. N   为所属类别C下的文档所有词出现的次数之和
			* 拉普拉斯平滑系数(解决P(F1|C) = 0 的问题)
				1. 计算方法: P(F1|C) = (Ni+a)/(N+am)
					* a为指定的系数一般为1
					* m为训练文档中统计出的特征词个数
		3. P(F1,F2..): 预测文档中每个词的概率
3. sklearn朴素贝叶斯实现API
	+ sklearn.naive_bayes.MultinomiaINB(alpha=1)
		1. alpha 拉普拉斯平滑系数---用来解决某词概率为0的问题
4. 朴素贝叶斯分类的优缺点
	+ 优点
		1. 朴素贝叶斯模型发源于古典数学模型，有稳定的分类效率
		2. 对缺失数据不太敏感，算法比较简单，常用于文本分类
		3. 分类准确性高，速度快
	+ 缺点
		1. 由于使用了样本属性独立性的假设，所以如果样本属性有关联，效果就不是很好
5. 分类模型的评估
	+ estimator.score()  使用准确率，即预测结果正确的百分比
	+ Precision          使用精确率，即预测结果为正例的样本中，真实结果为正例的比例(查的准)
	+ Recall             使用召回率，即真实结果为正例的样本中，测试结果为正例的比例(查的全，对正样本的区分能力)
	+ 混淆矩阵
		1.                    预测结果
		2. 真实结果              正例        假例
			+ 正例              真正例TP     伪反例FN
			+ 假例              伪正例FP     真反例TN
			+ T  True   
			+ F  False
			+ P  Positive
			+ N  Negative
	+ F1-score            反映模型的稳健型,值越大表示越稳健
	```
	F1 = 2TP/(2TP+FN+FP) 化简得  F1 = 2Precision*Recall/(Precision+Recall)
	```
	+ API
		1. sklearn.metrics.classification_report(y_true,y_pred,target_names)
		```
		print('当前评估报告为: ', classification_report(y_test,y_predict, target_names=news.target_names))
		```
		
#### 交叉验证
1. 定义: 将拿到的训练集数据，分为训练和验证集，将整体数据n等分，依次用其中的一份作为验证集
	+ 比如5等分，依次使用其中一份数据为验证集，就会得到5组模型，分别对5组模型求结果再取均值就是最终的验证值
	+ 又称为5折交叉验证
#### 超参数搜索----网格搜索(调参数)
1. 在通常情况下，有很多参数需要手动指定(如k-近邻苏算法中的K值)，这种叫做超参数
	+ 手动过程繁杂，所以需要对模型预设几种超参数组合
		1. 超参数不止一个时，需要使用排列组合，看一共有几种组合形式
	+ 每组超参数都采用交叉验证来进行评估
		1. 10折交叉验证
	+ 最终选择最优参数组合来建立模型
2. API
	+ sklearn.model_selection.GridSearchCV(estimator,param_grid=None,cv=None)
		1. CV 表示的是cross validation 交叉验证
		2. estimator 估计器对象
		3. param_grid 估计器参数  dict-->{'n_neighbors':[1,2,5]}
		4. cv  指定的几折交叉验证
		5. fit  输入数据
		6. score 准确率
		7. 结果分析
			+ best_score_: 在交叉验证中验证的最好结果
			+ best_estimator_: 最好的参数模型
			+ cv_results_: 每次交叉验证后的验证集和训练集的准确率结果

```
# -*- coding: utf-8 -*-
# __author__ : Ricky
# __createTime__ : 2019/11/24  20:24 
# __fileName__ : FBLocation.py
# __devIDE__ : PyCharm
import pandas as pd
from sklearn.model_selection import train_test_split,GridSearchCV
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler


def knncls():
    """
    K-近邻预测用户签到位置
    :return: None
    """

    # 读取数据
    data = pd.read_csv('./data/FBlocation/train.csv')
    print(data.head(10))

    # 处理数据
    # 首先 缩小数据  样本太大就减少样本量
    # 使用query查询数据筛选
    data = data.query("x>1.0 & x<1.25 & y>2.5 & y<2.75")

    # 对时间错进行处理  转换为日期格式，然后将年月日作为数据的新特征使用
    time_value = pd.to_datetime(data['time'], unit='s')
    print(time_value)
    # 将日期时间转换为 可以操作的字典格式
    time_value = pd.DatetimeIndex(time_value)

    # 构造一些特征
    data['day'] = time_value.day
    data['hour'] = time_value.hour
    data['weekday'] = time_value.weekday

    # 删除之前的时间戳特征  pd中的axis=1表示的是列
    data = data.drop(['time'], axis=1)
    print(data)
    # 把签到数量少于n个目标位置的删除
    # 按照列字段分组 列字段就会称为新数据的索引
    place_count = data.groupby('place_id').count()
    print(place_count)
    # reset_index  让索引称为特征列
    tf = place_count[place_count.row_id>3].reset_index()
    print(tf)
    # 取出符合条件的数据
    data = data[data['place_id'].isin(tf.place_id)]
    print(data)

    ### 预处理结束
    # 取出特征值和目标值
    y = data['place_id']
    x = data.drop(['place_id'], axis=1)

    # 进行数据的分割  训练集 测试集
    x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.25)

    # 特征工程(标准化) 对数据集进行标准化
    # std = StandardScaler()
    # x_train = std.fit_transform(x_train)
    # ## 这里直接使用transform就可以了 再fit一次就需要再次计算平均值和标准差 没必要
    # x_test = std.transform(x_test)
    # 进行k-近邻算法
    knn = KNeighborsClassifier()

    # # fit predict score
    # knn.fit(x_train,y_train)
    #
    # # 得出预测结果
    # y_predict = knn.predict(x_test)
    # print(y_predict)
    #
    # # 得出评估准确性
    # print(f'当前模型准确性为: {knn.score(x_test, y_test)}')
    params = {'n_neighbors': [3,5,7,10]}
    # fit predict score
    gcv = GridSearchCV(knn,param_grid=params, cv=5)
    gcv.fit(x_train, y_train)

    # 得出评估准确性
    print(f'当前模型准确性为: {gcv.score(x_test, y_test)}')

    # 在交叉验证中验证的最好结果
    print(f'当前交叉验证中验证的最好结果是: {gcv.best_score_}')
    print(f'当前交叉验证中验证的最好的参数模型是: {gcv.best_estimator_}')
    print(f'每次超参数组合交叉验证的结果是:: {gcv.cv_results_}')

    return None


if __name__ == '__main__':
    knncls()

```

### 分类算法----决策树、随机森林
1. 信息熵
	+ H(X) = P(x)logP(x)  (x表示的是在X特征下的各个类别)
	+ 单位是比特
	+ 在一无所知的情况下，信息熵最大，即付出的代价最大
		1. 举例: 32支球队夺冠的几率相同时，对应的信息熵等于5比特  log32=5(此时的信息熵也是最大的)
2. 信息和消除不确定性是相联系的
	+ 信息熵越大，表示不确定越大，预测付出的代价就越大
	+ 信息熵越小，表示已知性越大，预测付出的代价就越小

3. 决策树的分类依据之一
	+ 信息增益
		1. 当得知一个特征条件之后，减少的信息熵的大小
	+ 公式
		1. g(D|A) = H(D) - H(D|A)   
			+ A表示的是特征列
			+ 举例
				* g(D|年龄) = H(D) - H(D'|年龄)
					= H(D) - [1/3H(青年)+1/3H(中年)+1/3H(青老年]
					
4. 常见的决策树使用的算法
	+ ID3  信息增益  最大的准则
	+ C4.5 信息增益比  最大的准则
	+ CART 
		1. 回归树:  平方误差 最小
		2. 分类树:  基尼系数  最小的准则  
			* 在sklearn中可以选择划分的原则
			* 基尼系数是sklearn中的默认划分规则，划分得更加细
5. API
	+ sklearn.tree.DecisionTreeClassifier(criterion='gini', max_depth=None,random_state=None)
		1. 决策树分类器
		2. criterion  默认是"gini"系数，也可以选择信息增益的熵"entropy"
		3. max_depth  树的深度大小
		4. random_state  随机数种子
		5. 返回值
			* decision_path  返回决策树的路径
6. 小案例--泰坦尼克号船员预测---[数据链接](http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic.txt)
	+ pd读取数据
	+ 选择有影响的特征，处理缺失值
	+ 进行特征工程，pd转换字典，特征抽取  x_train.to_dict(orient='records')
		1. orient='records'  表示的是按行转换，每一行都会将列名作为key，对应的值为value
		2. 形式为 [{每一行转换后的字典},{每一行转换后的字典},{每一行转换后的字典}....]
	+ 决策树估计器流程
```
# -*- coding: utf-8 -*-
# __author__ : Ricky
# __createTime__ : 2019/11/26 9:34
# __fileName__ : Anaconda_study decisionTree.py
# __devIDE__ : PyCharm

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.feature_extraction import DictVectorizer



def decision():
    """
    使用决策树对泰坦尼克号船员获救进行预测
    :return:
    """
    ### 读取数据
    titan = pd.read_csv('http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic.txt')
    ### 处理数据 找出特征值和目标值
    x = titan[['pclass','age','sex']]
    y = titan['survived']

    ### 处理数值类型的缺失值
    x['age'].fillna(x['age'].mean(), inplace=True)
    print(x)
    ### 分割数据集到训练集和测试集
    x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.25)

    ### 进行处理（特征工程）  特征是类别的使用字典特征抽取将其表示为one-hot编码
    dict = DictVectorizer(sparse=False)
    x_train = dict.fit_transform(x_train.to_dict(orient='records'))
    x_test = dict.transform(x_test.to_dict(orient='records'))
    print(dict.get_feature_names())
    print(x_train)
    ### 使用决策树进行预测
    dec = DecisionTreeClassifier()
    #### fit
    dec.fit(x_train, y_train)
    ### 预测准确率
    print(f'预测的准确率为: {dec.score(x_test, y_test)}')



if __name__ == '__main__':
    decision()
```
7. 决策树的结构、本地保存
	+ sklearn.tree.export_graphviz(estimator, out_file='tree.dot',feature_names=) 
		1. 该函数能够导出DOT格式
	+ 工具:(可以将dot文件转换为pdf、png)
		1. 安装graphviz
		2. 运行命令  dot -Tpng tree.dot -o tree.png

8. 决策树的优缺点以及改进
	+ 优点
		1. 简单的理解和解释，树木可视化
		2. 需要很少的数据准备，其他算法通常需要进行数据归一化或者标准化
	+ 缺点
		1. 决策树学习者可以创建但是不能很好地推广数据--过于复杂的树
		2. 即训练集考虑进去的异常点可能对训练集效果是90+，但是可能导致测试集不走这些异常点的判定，效果可能只有70+，这种现象称为过拟合
	+ 改进
		1. 减枝cart算法
		2. 随机森林
9. window系统sklearn决策树graphviz绘图中文乱码解决方法
	+ 安装graphviz---请看scapy嗅探笔记
	+ 修改graphviz的字体配置文件
		1. 打开graphviz的安装路径--如‪D:\Program Files (x86)\Graphviz2.38\etc\fonts\fonts.conf
		2. 修改如下代码
		```
		### 原来的代码
		<!-- Font directory list -->
			<dir>#WINDOWSFONTDIR#</dir>
			<dir>~/.fonts</dir>
		### 修改后的代码
		<!-- Font directory list -->
			<dir>C:/windows/fonts</dir>
			<dir>~/.fonts</dir>
		```
	+ 修改python的sklearn源码
		1. 找到export.py源码------如D:\ProgramData\Anaconda3\Lib\site-packages\sklearn\tree\export.py
		2. 修改代码
		```
		### 原来的代码
		out_file.write('node [shape=box')
		### 修改后的代码
		out_file.write('node [fontname="Microsoft YaHei" shape=box')
		```
	+ windows系统中文字体的英文名[可选]
		1. 黑体：SimHei
		2. 宋体：SimSun
		3. 新宋体：NSimSun
		4. 仿宋：FangSong
		5. 楷体：KaiTi
		6. 仿宋_GB2312：FangSong_GB2312
		7. 楷体_GB2312：KaiTi_GB2312
		8. 微軟正黑体：Microsoft JhengHei
		9. 微软雅黑体：Microsoft YaHei
10. pydotplus的简介、安装、使用
	+ 安装   pip install pydotplus
	+ 使用
	```
	graph = pydotplus.graph_from_dot_data(dot_data)
	display(Image(graph.create_png()))
	```
### 随机森林---集成学习方法
1. 集成学习方法
	+ 定义: 通过建立几个模型组合来解决单一预测问题
	+ 原理: 生成多个分类器/模型，各自独立学习和作出预测
	+ 这些预测最后结合成单预测，因此优于任何一个单分类预测
2. 随机森林
	+ 定义: 在机器学习中，随机森林是一个包含多个决策树的分类器，并且其输出的类别是由个别树输出类别的众数而定的
		1. 比如5个决策树，4个为True，1个为False，则随机森林的最终结果是True
	+ 建立多个决策树的过程(多个树重复单个树的建立过程)
		1. N个样本，M个特征
		2. 单个树建立过程
			+ 随机在N个样本当中选择一个样本，重复N次，样本是有可能重复的(随机又放回的抽样--bootstrap)
			+ 随机在M个特征当中选出m个特征   m取值
		3. 建立多个决策树，样本、特征大多不一样
	+ 为什么要随机抽样训练集
		1. 如果不进行随机抽样，每棵树的训练集都一样，那么最终的训练结果也是完全一样好的
	+ 为什么要有放回地抽样
		1. 如果不放回地抽样，那么每棵树的训练样本都是不同的，都是没有交集的
		2. 这样每一棵树都是偏的，都是绝对的片面
		3. 也就是说每棵树训练出来都是有很大差异的
		4. 随机森林最后的分类取决于多棵树(弱分类器)的投票表决
	+ API 
		1. sklearn.ensemble.RandomForestClassifier(n_estimators=10,criterion='gini',max_depth=None,bootstrap=True,random_state=None)
			* 随机森林分类器
			* n_estimators  默认是10  森林里树木数量 120,200,300,500,800,1200
			* criterion    分割特征的测量算法
			* max_depth    树的最大深度，默认无  5,8,15,25,30
			* max_features 每个决策树的最大特征数量(特征太多容易过拟合)
				1. auto  max_features = sqrt(n_features)
				2. sqrt  同上
				3. log2  max_features = log2(n_features)
				4. None  max_features = n_features
			* bootstrap   默认是True  是否在勾建树时使用随机又放回式抽样
```
def randomForest():
    """
    使用随机森林对泰坦尼克号船员获救进行预测
    :return:
    """
    ### 读取数据
    titan = pd.read_csv('http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic.txt')
    ### 处理数据 找出特征值和目标值
    x = titan[['pclass', 'age', 'sex']]
    y = titan['survived']

    ### 处理数值类型的缺失值
    x['age'].fillna(x['age'].mean(), inplace=True)
    print(x)
    ### 分割数据集到训练集和测试集
    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25)

    ### 进行处理（特征工程）  特征是类别的使用字典特征抽取将其表示为one-hot编码
    dict = DictVectorizer(sparse=False)
    x_train = dict.fit_transform(x_train.to_dict(orient='records'))
    x_test = dict.transform(x_test.to_dict(orient='records'))
    print(dict.get_feature_names())
    print(x_train)


    ### 使用随机森林进行预测
    rf = RandomForestClassifier()
    params = {'max_depth': [5,8,15,25,30], 'n_estimators': [120,200,300,500,800,1200]}
    ### 网格搜索与交叉验证
    gcv = GridSearchCV(estimator=rf, param_grid=params,cv=2)
    #### fit
    gcv.fit(x_train, y_train)
    ### 预测准确率
    print(f'预测的准确率为: {gcv.score(x_test, y_test)}')

    ### 最优模型组合
    print(f'最优的模型参数组合: {gcv.best_params_}')
```
3. 随机森林的优点
	+ 在当前所有算法中，具有极好的准确率
	+ 能够有效地运行在大数据集上
	+ 能够处理具有高维特征的输入样本，而且不需要降维
	+ 能够评估各个特征在分类问题上的重要性

## 回归算法
### 线性回归-----特征需要标准化
1. 线性回归其实就是在找一种能预测的趋势
2. 线性关系
	+ 二维: y=kx+b  直线型  单特征x  b是偏移项，使得该模型更加通用  
	+ 三维:         平面型  多特征 
3. 线性关系模型
	+ 一个通过属性(特征)的线性组合来进行预测的函数
	+ f(x) = W1X1+W2X2+W3X3+....+WdXd+b
		* w 为权重
		* b 为偏置项  可以理解为 W0X0(X0等于1)
4. 线性回归
	+ 定义: 线性回归通过一个或者多个自变量与因变量之间进行建模的回归分析
		1. 可以为一个自变量的线性组合
		2. 也可以为多个自变量的线性组合
	+ 一元线性回归
		1. 涉及的变量是一个
	+ 多元线性回归
		1. 涉及的变量是两个以上
	+ 通用公式
		1. h(W) = W0+W1X1+W2X2+...   (属性和权重的相互组合来预测结果)
				= wTx
				
		2. 其中w、x为矩阵: w = W0 W1 W2 (权重) | x = X1 X2 X3 (属性即特征)

5. 损失函数(误差大小)
	+ Yi为第i个训练样本的真实值
	+ Hw(Xi)为第i个训练样本特征值组合预测函数
	+ 总损失定义
		1. J(θ) = (Hw(Xi)-Yi)的平方和 i<m  最小二乘法    
		2. 总损失越小，说明训练样本的真实值与测试值的误差就越小

6. 如何去求模型中的w权重值，使得损失最小，即误差最小，最接近真实值	
	+ 最小二乘法之正规方程
		1. 求解  w = (X的转置*X)的逆*X的转置*y
			* X为特征值矩阵，y为目标值矩阵
		2. 缺点
			* 当特征过于复杂时，求解速度太慢
	+ 梯度下降(即寻找损失函数的最低点---比作山谷)
		1. 计算下山最快的方向
		2. 下山的速度(即学习速率)
		3. 找到山谷时，w值就是使得损失函数最小的权重参数
7. sklearn 线性回归正规方程与梯度下降的API
	+ sklearn.linear_model.LinearRegression  正规方程
	+ sklearn.linear_model.SGDRegression     梯度下降
	+ 返回值都是coef_ : 回归系数即w权重参数
	+ sklearn  版本
		1. 0.18版本  线性回归传入的参数可以是一维的或者二维的
		2. 0.19版本                  强制为二维的
8. 波士顿房价预测
```
# -*- coding: utf-8 -*-
# __author__ : Ricky
# __createTime__ : 2019/11/26  23:38 
# __fileName__ : LineRegressor.py
# __devIDE__ : PyCharm


from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression, SGDRegressor
from sklearn.datasets import load_boston
from sklearn.metrics import mean_squared_error

def myLinearRegressor():
    """
    线性回归直接预测房价
    :return:
    """
    # 读取数据
    bos = load_boston()

    # 分割数据
    x_train,x_test,y_train,y_test = train_test_split(bos.data, bos.target, test_size=0.25)

    # 对特征值和目标值分别进行标准化
    std_x = StandardScaler()
    x_train = std_x.fit_transform(x_train)
    x_test = std_x.transform(x_test)

    std_y = StandardScaler()
    y_train = std_y.fit_transform(y_train.reshape(-1,1))
    y_test = std_y.transform(y_test.reshape(-1,1))

    # 正规方程线性回归预测
    lr = LinearRegression()

    # fit
    lr.fit(x_train, y_train)

    # 预测后得出的w权重参数值
    print(f'正规方程得出的w参数为: {lr.coef_}')

    # 对测试集预测的房价
    y_pred = std_y.inverse_transform(lr.predict(x_test))
    print(f'使用w参数对测试集房价进行预测: {y_pred}')


    print(f'正规方程的均方误差: {mean_squared_error(std_y.inverse_transform(y_test), y_pred)}')
    # 梯度下降线性回归预测
    sgd = SGDRegressor()

    # fit
    sgd.fit(x_train, y_train)

    # 预测后得出的w权重参数值
    print(f'正规方程得出的w参数为: {sgd.coef_}')

    # 对测试集预测的房价
    y_pred = std_y.inverse_transform(sgd.predict(x_test))
    print(f'使用w参数对测试集房价进行预测: {y_pred}')

    print(f'梯度下降的均方误差: {mean_squared_error(std_y.inverse_transform(y_test), y_pred)}')


if __name__ == '__main__':
    myLinearRegressor()



### 解析
	+ 需要分别对特征值和目标值进行标准化，即实例化两个标准化对象(因为特征值与目标值类型不一样，没法使用前者计算号的均值和方差来进行标准化)
	+ 在预测结果时，由于前面已经做了标准化，所以预测结果是均值为0，标准差为1的数值，可以使用之前的标准化对象进行恢复
		1. std_y.inverse_transform(预测值)
		
```
9. 回归性能评估
	+ 均方误差 MSE(Mean Squuared Error)
		1. MSE = (预测值-真实值)的平方和的平均值
	+ API
		1. sklearn.mean_squared_error(y_true,y_pred)
			* 均方误差回归损失
			* y_true   真实值          标准化之前的值
			* y_pred   预测值          标准化之前的值
			* return   浮点数结果
10. 梯度下降与正规方程的对比
	+ 梯度下降                      正规方程
	+ 需要选择学习率                  不需要
	+ 需要多次迭代                 一次运算得出
	+ 当特征数量n大时也能较好适用      需要计算(X的转置*X)的逆
								1. 如果特征数量n较大时则运算代价大
								2. 因为矩阵逆的计算时间复杂度为O(n的三次方)
								3. 通常来说当n小于10000时还是可以接受的
	+ 适用于各种类型的模型            只适用于线性模型，不适合逻辑回归模型等其他模型
	+ 适用于大规模数据(10万+)          适合小规模数据，不能解决拟合问题
11. 训练数据集训练得很好，误差也不大，但是用在测试集上，效果却不那么好
	+ 过拟合
		1. 假设在训练数据集上能够获得比其他假设更好的拟合
		2. 但是在训练集以外的数据集上却不能很好地拟合数据
		3. 此时认为这个假设出现了过拟合现象(模型过于复杂)
	+ 欠拟合
		1. 假设在训练数据上不能获得更好的拟合
		2. 但是在训练集之外的数据集上也不能很好地拟合数据
		3. 此时认为这个假设出现了欠拟合现象(模型过于简单)
12. 简单模型与复杂模型
	+ 模型复杂的原因: 数据的特征和目标之间的关系不仅仅是线性的，很可能还是非线性的
	+ 简单模型: 线性的
	+ 复杂模型: 非线性的
13. 欠拟合和过拟合的原因以及解决方法
	+ 欠拟合
		1. 原因: 学习到数据的特征过少
		2. 解决方法: 增加数据的特征数量
	+ 过拟合
		1. 原因: 原始特征过多，存在一些嘈杂的特征(即异常点过多表现出的病态特征)，模型过于复杂是因为模型尝试去兼顾各个测试点
		2. 解决方法
			* 进行特征选择，消除关联性大的特征(特别难做)
			* 交叉验证(让所有的数据都有过训练)
			* 正则化
	+ 欠拟合和过拟合是根据现象去判断的
		1. 训练集表现好，测试集表现不好       过拟合
		2. 训练集表现不好，测试集表现也不好   欠拟合
14. 特征选择
	+ 过滤式: 低方差特征
	+ 嵌入式: 正则化、决策树、神经网络
15. L2正则化----解决模型复杂导致的过拟合现象
	+ 背景: 模型复杂的原因是某些特征的表现形式过于复杂(高次幂函数)
	+ 作用: 可以使得某些复杂特征的权重W趋渐为0，从而尝试消除高次项元素，看看对最后的结果的影响程度
	+ 优点: 越小的参数说明模型越简单，越简单的模型则越不容易产生过拟合现象
16.    算法           策略              优化
	+ 线性回归       损失函数

### 岭回归---带有L2正则化的线性最小二乘法
1. sklearn.linear_model.Ridge(alpha=1.0)
	+ alpha  正则化力度  
		1. 超参数，可以通过网格搜索和交叉验证进行调参
		2. 范围大小: 一般是0~1之间的小数  1~10之间的整数
	+ coef_  回归系数
	+ 正则化力度: alpha越大，w权重系统越趋于0，即使得模型越简单


2. 线性回归与岭回归的对比
	+ 岭回归
		1. 回归得到的回归系数更符合实际，更可靠
		2. 能够让估计参数的波动范围变小，变得更加稳定(L2正则化)
		3. 在存在病态数据(异常数据)偏多的研究中有较大的实用价值

### 模型的保存与加载
1. from sklearn.externals import joblib
	+ 保存: joblib.dump(rf,'test.pkl')
	+ 加载: joblib.load('test.pkl')  返回值为保存时的模型，这里是rf
```
# 岭回归线性回归预测
rd = Ridge()

# fit
rd.fit(x_train, y_train)

### 使用sklearn保存数据模型
joblib.dump(rd, 'test.pkl')
# 预测后得出的w权重参数值
print(f'正规方程得出的w参数为: {rd.coef_}')
```

### 逻辑回归
1. 应用场景
	+ 广告点击率
	+ 是否为垃圾邮件
	+ 是否患病
	+ 金融诈骗
	+ 虚假账号
2. 经典的二分类算法，特点是可以得出具体的概率值
	+ 哪个类别少，就判定哪个类别的概率值，超过阈值的就是该类别，反之则为另一个类别

3. 通用公式
	+ Z(W) = W0+W1X1+W2X2+...   (属性和权重的相互组合来预测结果)
			= wTx
			
    + 其中w、x为矩阵: w = W0 W1 W2 (权重) | x = X1 X2 X3 (属性即特征)
4. 逻辑回归与线性回归的关系
	+ 逻辑回归与线性回归的数学表达式是相同的
	+ 逻辑回归就是将线性回归的输入转换为某种二分类问题
5. 逻辑回归实现二分类的流程
	+ sigmoid函数  完成将输入的值映射为0~1之间的概率值，中间值默认为0.5
		1. g(z) = 1/(1+e的-z次幂)
		2. 输出: [0,1]区间的概率值，默认0.5为阈值
		3. z为线性回归的结果
	+ 与线性回归原理相同，但是损失函数不一样，只能通过梯度下降求解
		1. 对数似然损失函数  
		2. 比如有4个样本，样本损失总和为 (真实值*log测试值)求和
			
6. 损失函数(梯度下降求解)
	+ 均方误差(不存在多个局部的最低点)，只有一个最小值
	+ 对数似然损失     多个局部最小值，目前解决不了
		1. 多次随机初始化位置，多位置寻找最低点，多次比较最小值结果
		2. 调整学习率
7. API
	+ sklearn.linear_model.LogisticRegression(penal='l2',C=1.0)
		1. Lofistic       回归分类器
		2. coef_          回归系数
		3. penal='l2'     表示的是正则化
		4. C=1.0          表示正则化的力度是1.0
8. 小案例----癌症预测
[数据链接](http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/)
```
# -*- coding: utf-8 -*-
# __author__ : Ricky
# __createTime__ : 2019/11/28  0:31 
# __fileName__ : logistic.py
# __devIDE__ : PyCharm

from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report
from sklearn.model_selection import train_test_split
import pandas as pd
import numpy as np


def logisticRegressor():
    """
    使用逻辑回归作二分类进行癌症预测
    :return:
    """
    ### 构造列标签名字
    column = ['Sample code number','Clump Thickness','Uniformity of Cell Size','Uniformity of Cell Shape','Marginal Adhesion','Single Epithelial Cell Size','Bare Nuclei','Bland Chromatin','Normal Nucleoli','Mitoses','Class']
    ### 使用pandas读取网络数据
    data = pd.read_csv('./data/Logistic/breast-cancer-wisconsin.data', names=column)


    ### 对缺失值进行处理，将?替换为np.nan
    data = data.replace(to_replace='?', value=np.nan)
    data = data.dropna()
    # print(data)
    ### 分割训练集和测试集
    x_train,x_test,y_train,y_test = train_test_split(data[column[1:10]], data[column[-1]],test_size=0.25)

    ### 进行标准化进行处理
    std = StandardScaler()
    x_train = std.fit_transform(x_train)
    x_test = std.transform(x_test)

    ### 初始化逻辑回归
    lg = LogisticRegression()
    lg.fit(x_train, y_train)
    y_pred = lg.predict(x_test)
    print(f'当前预测准确率为:{lg.score(x_test, y_test)}')

    print(f'当前测试的回归参数: {lg.coef_}')

    ### 得出分类问题中的召回率指标
    print(f'当前模型预测的召回率是: {classification_report(y_test, y_pred)}')


if __name__ == '__main__':
    logisticRegressor()


```
9. 逻辑回归总结
	+ 应用
		1. 广告点击率
		2. 是否患病
		3. 金融诈骗
		4. 是否为虚假账号
	+ 优点
		1. 适合需要得到一个分类概率的场景
		2. 简单、速度快
	+ 缺点
		1. 不好处理多分类问题
	+ 其他用途
		1. softmax方法----逻辑回归在多分类问题上的推广(神经网络算法)
	
10. 逻辑回归和朴素贝叶斯对比
	+              逻辑回归                朴素贝叶斯
	+ 解决问题       二分类                  多分类
	+ 应用场景  癌症、需要概率的二分         文本分类
	+ 参数          正则化力度                没有
	+ 模型           判别模型                  生成模型
11. 判别模型和生成模型的判断
	+ 生成模型具有先验概率，而判别模型没有
		1. 什么是先验概率? 即需要从历史数据中总结出概率信息
		2. 朴素贝叶斯中的P(C)指的就是所属当前类别的文档数在总文档数中的概率(是需要提前计算总结的)

12. 练习题
[数据来源](https://jdder.jd.com/)

## 非监督学习
1. 特点: 只有特征值，没有目标值
2. 主要应用: 聚类(如k-means)
3. 举例说明聚类的流程(比如有1000个样本，需要分为k=3类)
	+ 随机在数据中抽取三个样本，当作三个类别的中心点(K1,K2,K3)
	+ 计算其余点分别到该三个中心点的距离
		1. 每一个样本就会有三个距离
		2. 从中选出距离最近的那个中心点作为当前点的标记
		3. 这样就会形成三个族群
	+ 分别计算这三个族群的平均值
		1. 把三个平均值与之前的三个旧中心点进行比较
			* 如果相同: 结束聚类
			* 如果不相同: 把这三个平均值作为新的中心点，然后重复第二步
4. k-means步骤
	+ 随机设置k个特征空间内的点作为初始的聚类中心
	+ 对于其他每一个点计算到K个中心点的距离，未知点选择最近的一个中心点作为标记类别
	+ 接着对着中心点进行聚类标记，重新计算每一个聚类的新中心点(取聚类后的每一个点的坐标差的均值与原先的中心点比较)
	+ 如果计算的新中心点与原中心点一样，那么结束，否则将新中心点作为聚类标记，重复以上步骤
	
5. API
	+ sklearn.cluster.KMeans(n_cluster=8,init='k-means++')
		1. k-means聚类
		2. n_cluster: 开始的聚类中心点数量
		3. init: 初始化方法，默认为"k-means++"
		4. label_: 默认标记的类型，可以和真实值比较(不是值比较)
6. k-means聚类的评估标准
	+ 轮廓系数
		1. sc_i = (b_i-a_i)/max(b_i,a_i)
			* b_i为i到其他族群所有点的距离均值中的最小值
				1. 假设其他族群为2，3 则有b_i_2 b_i_3取其中的最小值
			* a_i为i到自身类别其他所有点的距离的均值
				
		2. b_i>>a_i  趋近于1，聚类效果最好
		3. b_i<<a_i  趋近于-1，聚类效果最差
		4. 轮廓系数的取值介于[-1,1]  一般大于0.5就算好的
			* 趋近于1代表内聚度和分离度都相对较优
			* 即内部距离小，外部距离大(高内聚)
		5. API
			* sklearn.metrics.silhouette_score(X,labels)
				1. 计算所有样本的平均轮廓系数
				2. X : 特征值
				3. labels: 被聚类标记的目标值

7. k-means总结
	+ 特点分析
		1. 采用迭代式算法
		2. 直观易懂并且非常实用
	+ 缺点
		1. 容易收敛到局部最优解，即初始化时，中心点被随机到某一个局部区域
		2. 解决方法: 多次聚类(一次一次迭代，寻找最优的中心点)
		

# 深度学习
## Tensorflow基础
0. 安装Tensorflow1.0 CPU版本
	+ Python必须是3.5版本的
	+ [链接地址](https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-1.0.0-cp35-cp35m-win_amd64.whl)
	+ pip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-1.0.0rc2-cp35-cp35m-win_amd64.whl
	+ 版本选择
		1. tensorflowV1.1 + NumpyV1.2+(不要最新版本) 可以解决1.0.0版本带来的问题
		```
		OpKernel ('op: "UpdateFertileSlots" device_type: "CPU"') for unknown op: UpdateFertileSlots
		```
1. 机器学习与深度学习对比
	+ 算法不同
		1. 机器学习: 分类，回归
		2. 深度学习: 神经网络
			* 卷积神经网络  图像识别
			* 自然语言处理  循环神经网络
	+ 领域不同
2. Tensorflow特点
	+ 真正的可移植性
		1. 支持CPU/GPU/TPU
		2. 很好地运行在移动端，如安卓、IOS、树莓派
	+ 多语言支持
		1. TensorFlow有一个c++使用界面
		2. 也有Python界面
	+ 高度的灵活性与效率
		1. TensorFlow是一个采用数据流图(data flow graphs)
		2. 用于数值计算
		3. 可以灵活进行组装图、执行图
	+ 支持
		1. TensorFlow由谷歌团队支持
3. TensorFlow基础
	+ 关闭警告
	```
	import os
	os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
	```
	+ tensor 张量  数据的结构
	+ operation(op)   专门运算的操作节点
	+ graph  图        程序的整个结构
	+ Session  会话    运算程序的图
4.       计算密集型                 IO密集型
	+    tensorFlow             磁盘IO，网络IO
 
5. 图
	+ 当前整个程序就是一个图(默认已经注册的)
		1. 当前图中包含在该图中定义的tf.Operation操作节点对象以及tf.Tensor数据单元对象
		2. tf.get_default_gragh()  获取当前程序图  
			* 打印出来的是内存地址  表示存储当前图中所有对象的内存地址
	```
	import tensorflow as tf
	import os
	os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
	a = tf.constant(5.0)
	b = tf.constant(6.0)
	gragh = tf.get_default_graph()
	print(gragh)
	sum1 = tf.add(a, b)
	with tf.Session() as sess:
		print(sess.graph)
		print(sess.run(sum1))	
	```
	+ 图的创建
		1. 本质就是开辟一个新的内存空间去操作数据---有点像函数式编程
		2. 如何保证在开辟的空间操作----当前上下文环境(类似函数内部的函数体)
		```
		import tensorflow as tf
		import os
		os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
		g = tf.Graph()
		print(g)
		with g.as_default():
			a = tf.constant(10)
			print(a.graph)
		```
		
6. op和tensor
	+ op       凡是使用tensorFlow的API定义的函数都是op
	+ tensor   指代的仅仅是数据(数据的类型)
	+ tensor和op的关系
		1. op是tensor的载体
		2. tensor是op载体中所包含的数据的表现形式(张量)
7. tensorFlow的前后端系统
	+ 前端系统   定义程序的图结构
	+ 后端系统   运算图的各个结构
8. 会话
	+ 运行图的结构(只能运行一张图，可以通过gragh参数去指定)
	+ 分配计算资源(CPU/GPU/TPU等)
	+ 掌握着各种资源(释放队列、线程、变量等资源--tensorFlow中的资源)
	+ 使用config=tf.ConfigProto(log_device_placement=True)
		1. 打印出当前上下文(即默认注册的图)里的各种资源
		2. 打印出当前图的结构的计算资源
		```
		import tensorflow as tf
		import os
		os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
		a = tf.constant(20)
		b = tf.constant(10)
		sum1 = tf.add(a,b)
		with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:
			sess.run(sum1)
			print(sess.graph)
			
		### 输出结果
		Device mapping: no known devices.
		Add: (Add): /job:localhost/replica:0/task:0/cpu:0
		Const_1: (Const): /job:localhost/replica:0/task:0/cpu:0
		Const: (Const): /job:localhost/replica:0/task:0/cpu:0
		<tensorflow.python.framework.ops.Graph object at 0x00000194AAE58940>
		```
	+ 交互式(主要用在命令行)----tf.InteractiveSession()
		1. 在命令行中不使用session
		2. 对于张量类型的数据，直接使用xx.eval()就可以打印出xx的值
	+ eval()可以在某个图的上下文中直接使用
		1. 交互式命令行
		2. with tf.Session() as sess
9. 会话的run(fetches,feed_dict=None,gragh=None)方法
	+ 运行ops和计算tensor
	+ run多个op时，需要以列表的形式
	+ feed_dict的使用----主要应用场景是实时处理数据(数据结构是动态变化的)
		1. 首先使用plt = tf.placeholder([None,3]) 进行数据占位(表示的是多行3列数据)
		2. feed_dict={"plt": data}
			```
			import tensorflow as tf
			import os
			os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
			plt = tf.placeholder(tf.float32, [None, 3])
			with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:
				print(sess.run(plt, feed_dict={plt: [[1,2,3], [4,5,6]]}))
				print(sess.graph)
			```
	+ 返回值异常
		1. RuntimeError  如果它Session处于无效状态(例如已关闭)
		2. TypeError     如果fecthes或feed_dict键是不合适的类型
		3. ValueError    如果fetches或feed_dict键无效或引用Tensor不存在
10. 运算符重载
	+ 当Tensor形式的数据与Python中某数据类型的数据产生基本运算时
	+ 此时运算符被重载，比如 int+tensor 本质是tf.add(int,tensor)
	+ 最终的计算结果也是Tensor形式的数据

### 张量
1. 数组、矩阵和张量
	+ 数组   1、2、3维...  numpy.ndarray类型
	+ 矩阵   必须是2维     numpy.dot()
	+ 张量   TensorFlow是基于Numpy计算的
		1. 本质是对numpy数据类型的封装
		2. 数据类型为tensor
2. 张量的数据类型
	+ 一个类型化的N维数组 tf.Tensor
	+ 包括三部分 --- Tensor()
		1. 名字
		2. 形状
		3. 数据类型
	+ 常用的数据类型
		1. DT_FLOAT          tf.float32
		2. DT_DOUBLE         tf.float64
		3. DT_INT64          tf.int64
		4. DT_INT32          tf.int32
		5. DT_INT8           tf.int8
		6. DT_UINT8          tf.uint8
		7. DT_STRING         tf.string
			* 可变长度的字节数组
			* 每一个张量元素都是一个字节数组
		8. DT_BOOL           tf.bool
3. 张量的阶
	+ 阶数      数学实例         Python      例子
	+ 0         纯量            只有大小     s=45                     shape=()
	+ 1         向量            大小和方向   v=[1.1,1.2]              shape=(2)
	+ 2         矩阵            数据表       m=[[1,2,3]]              shape=(1,3)
	+ 3         3阶张量          数据立体    t=[[[2],[3]],[[5],[6]]]  shape=(1,2,2)
	+ n....
4. 张量的动态形状与静态形状
	+ 静态形状
		1. 创建一个张量，初始状态的形状
		2. tf.Tensor.get_shape 获取静态形状
		3. tf.Tensor.set_shape() 更新Tensor对象的静态形状
			* 使用场景: 当tensor对象的形状并没有完全固定时，比如placeholder([None,7])
			* 占位符行数不固定，列数固定，所以set_shape()是可以改变行数的
		4. 对于静态形状来说，一旦张量的形状固定了，不能再次设置静态形状
	+ 动态形状
		1. 一种描述原始张量在执行过程中的一种形状(动态变化)
		2. tf.reshape   创建一个具有不同动态形状的新张量
#### 张量操作API
1. 生成张量
	+ tf.zeros(shape)
	+ tf.ones(shape)
2. 正态分布(高斯分布)----现实生活中大多数随机现象更符合正态分布
	+ 概率密度函数的位置由正态分布的期望值(均值)决定
	+ 分布的幅度由标准差决定(y值)
		1. 标准差越大，表示越分散，幅度越小
		2. 标准差越小，表示越密集，幅度越大
	+ 分布特点
		1. 期望值=0，标准差=1时的正态分布是标准正态分布
		2. 95.45%的面积在平均数左右2个标准差范围内
		3. 99.73%                3
		4. 99.99%                4
	+ 生成符合正态分布的随机值API
		1. tf.random_normal(shape,mean=0.0,stddev=1.0)
			* mean    表示的是期望值(均值)
			* stddev  表示的是标准差
3. 改变张量元素的数据类型
	+ 常用tf.cast(x,dtype,name=None)
		1. x 原始数据
		2. dtype 要转换的数据类型
4. 切片和扩展
	+ tf.concat(values,axis=?)
		1. values 表示的是要拼接合并的数据集 [a,b]
		2. axis   表示的是将数据按照哪个维度进行合并
[矩阵操作API](https://github.com/tensorflow/docs/blob/r1.2/site/en/api_docs/python/tf.md)
### 变量
1. 变量也是一种OP，是一种特殊的张量，能够进行存储持久化，它的值就是张量，默认被训练
	+ 一般存放模型训练后得出的参数系数
2. 变量的创建以及初始化
```
import tensorflow as tf
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'


a = tf.constant(10.0)
b = tf.Variable(initial_value=tf.random_normal(shape=(2,3), mean=0.0, stddev=1.0, dtype=tf.float32))
print(a, b)
init_var = tf.global_variables_initializer()

with tf.Session() as sess:
    sess.run(init_var)
    sess.run([a,b])
```
3. 变量的特点
	+ 变量op能够持久化保存，普通张量op是不行的
	+ 当定义一个变量op的时候，一定要在会话当中去运行初始化
		1. 初始化变量API生成的op，需要在session中进行run
4. op的name参数
	+ 主要是用在tensorboard可视化上时，给相同的op取各自的名称
	+ 区分相同的op，打印更直观
		
#### 可视化学习--Tensorboard
1. 序列化图的结构--生成events文件
	+ Tensorboard通过读取TensorFlow的事件文件夹来运行
2. tf.summary.FileWriter('//', gragh=xx)
	+ 返回writer，写入图结构到事件文件(最好绝对路径)
	+ 该操作一般在session下操作，因为此时图结构基本完整
	+ summary  摘要，总结   主要用作后台管理的操作(Web界面)
3. 开启TensorBoard
	+ tensorboard --logdir="evnets事件文件路径"
	+ 一般浏览器会默认打开127.0.0.1:6006
4. 修改程序后，再保存一遍会生成新的事件文件，打开默认为最新
5. 面板上一些常用的选项卡
	+ SCALARS     0维度值   准确率、损失等
	+ GRAGHS      显示程序的结构
	+ HISTOGRAMS  高纬度的值  >0维度的值  权重参数列表  偏置
6. 使用Tensorboard可视化遇到的坑
	+ [How to compile Tensorflow with SSE4.2 and AVX instructions?](https://stackoverflow.com/questions/41293077/how-to-compile-tensorflow-with-sse4-2-and-avx-instructions)
	+ No graph definition files were found(使用Tensorboard可视化时一定要先打开Gragh选项卡查看是否能用----因为程序可能确实没有什么数据，但是一定有图)
		1. 首先看绝对路径是否正确  如: D://DP_Study//MyTensorflow//logs  ----路径不要出现中文和空格等特殊字符
		2. 其次使用相对路径  cd到logs的上一级目录  执行 tensorboard --logdir=logs [--host=127.0.0.1  可选]
		3. 重新安装TensorFlow
7. 当前使用Tensorboard可视化的环境
	+ pip install numpy==1.13
	+ pip install six==1.13
	+ pip install protobuf==3.6
	+ pip install --upgrade https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-1.1.0-cp35-cp35m-win_amd64.whl(最后安装，不然上面的依赖库都会自动下载最新版本)
		1. 在Linux上安装     linux/cpu/tensorflow-1.1.0-cp35-cp35m-linux_x86_64.whl
		2. 在Windows上安装   windows/cpu/tensorflow-1.1.0-cp35-cp35m-win_amd64.whl
		3. ....
	
8. 复习线性回归的大致流程
	+ 准备数据  特征值+目标值
	+ 建立模型  先随机给地权重参数W和偏置参数bias
	+ 求损失函数(这里就是计算真实值与预测值之间的误差)
		1. 均方误差   (真实值-预测值)平方和再求平均值
	+ 优化损失函数，找到最优解
		1. 梯度下降去优化损失过程，指定学习率
9. Tensoflow中线性回归算法涉及的API
	+ 矩阵计算
		1. tf.matmul(x,w)
	+ 平方
		1. tf.square(error)
	+ 均值
		1. tf.reduce_mean(error)
	+ 梯度下降
		1. tf.train.GradientDescentOptimizer(learning_rate)
			* learning_rate 学习率
			* 方法: minimize(loss)  最小损失，即达到误差最优
			* 返回值  梯度下降op
	+ 在算法中
		1. 不变的是特征值，用张量表示
		2. 变化的是特征值的权重，用变量表示
		3. 变量想要固定其值，只需要设置参数 trainable=False即可
```
# -*- coding: utf-8 -*-
# __author__ : Ricky
# __createTime__ : 2019/12/3  19:45 
# __fileName__ : LinearRegression.py
# __devIDE__ : PyCharm

import tensorflow as tf
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

def LinearRegressor():
    # 创建数据
    x_train = tf.random_normal(shape=[100,1], mean=1.75, stddev=0.5, name='x_train')
    y_true = tf.matmul(x_train, [[0.7]]) + 0.8
    # 建立模型  一个特征所以一个权重，一个偏置
    weight = tf.Variable(initial_value=tf.random_normal(shape=[1,1]), name='w')
    bias = tf.Variable(initial_value=0.6, name='bias')
    y_pred = tf.matmul(x_train, weight) + bias

    # 建立损失函数。计算均方误差
    loss = tf.reduce_mean(tf.square(y_true-y_pred))

    # 梯度下降优化损失到最新  求最小损失
    train_op = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(loss)

    ### 变量初始化
    init_op = tf.global_variables_initializer()

    # 通过会话运行程序
    with tf.Session() as sess:
        ## 初始化变量
        sess.run(init_op)
        ## 打印出随机最先初始化的权重和偏置
        print("随机初始化的参数权重为: %f, 偏置为: %f" % (weight.eval(), bias.eval()))

        ## 循环训练 打印出每次学习的权重和偏置

        ## 定义训练的次数  step
        step = 100
        for i in range(step):
            sess.run(train_op)
            print("当前第%d次学习, 参数权重为: %f, 偏置为: %f" % (i+1,weight.eval(), bias.eval()))


if __name__ == '__main__':
    LinearRegressor()
```
10. 梯度爆炸/梯度消失
	+ 在极端情况下，权重的值变得非常大，以至于溢出，导致NaN值
	+ 如何解决该问题
		1. 重新设计网络
		2. 调整学习率
		3. 使用梯度截断(在训练过程中检查和限制梯度的大小)
		4. 使用激活函数
11. 变量的作用域--variable_scope
	+ 变量在指定上下文中有效
	+ 使得代码更清晰，Tensorboard显示也更加明了
```
with tf.variable_scope('data'):

	# 创建数据
	x_train = tf.random_normal(shape=[100,1], mean=1.75, stddev=0.5, name='x_train')
	y_true = tf.matmul(x_train, [[0.7]]) + 0.8
```

12. 增加变量显示---观察模型的参数、损失值等变量值的变化(就是将想要观察的参数放在Tensorboard中展示)
	+ 收集变量
		1. tf.summary.scalar(name=,tensor)
			* 收集对于损失函数和准确率等单值变量
			* name 变量的名称
			* tensor 值
		2. tf.summary.histogram(name=,tensor)
			* 收集高纬度的变量参数
			* 同上
		3. tf.summary.image(name=,tensor)
			* 收集输入的图片张量--显示图片
			* 同上
	+ 合并变量写入事件文件
		1. merged = tf.summary.merge_all()
		2. 运行合并: summary=sess.run(merged)
			* 每一次迭代都需要运行
		3. 添加: FileWriter.add_summary(summary,i)
			* i 表示第几次的值
```
### 收集变量
tf.summary.scalar('losses', loss)
tf.summary.histogram('weight', weight)

 ### 每一次训练 都要运行一次收集的变量
summary = sess.run(merged_op)
fileWriter.add_summary(summary, i)
```
13. 模型保存和加载
	+ tf.train.Saver(var_list=None, max_to_keep=5)
		1. var_list  
			* 指定要保存和还原的变量
			* 可以作为一个dict或者list传递
			* 为None则表示保存整个sess会话中的变量
		2. max_to_keep
			* 指示要保留的最近检查点文件的最大数量
			* 创建新文件时，会删除较旧的文件
			* 默认保留5个检查点文件，0或者None表示保留所有的检查点文件
		3. 保存文件格式  checkpoint文件
		4. 返回值  saver_op
	+ 模型保存: saver.save(sess,模型保存路径==路径+模型名)
	+ 模型加载: saver.restore(sess,路径+模型名)
```
### 定义一个保存模型的实例op
saver = tf.train.Saver()
# 通过会话运行程序
with tf.Session() as sess:
	## 初始化变量
	sess.run(init_op)
	## 打印出随机最先初始化的权重和偏置
	print("随机初始化的参数权重为: %f, 偏置为: %f" % (weight.eval(), bias.eval()))
	fileWriter = tf.summary.FileWriter('../logs/', graph=sess.graph)
	if os.path.exists('../models/checkpoint'):
		saver.restore(sess, '../models/test')

	## 循环训练 打印出每次学习的权重和偏置
	## 定义训练的次数  step
	step = 100
	for i in range(step):
		sess.run(train_op)
		### 每一次训练 都要运行一次收集的变量
		summary = sess.run(merged_op)
		fileWriter.add_summary(summary, i)
		print("当前第%d次学习, 参数权重为: %f, 偏置为: %f" % (i+1,weight.eval(), bias.eval()))

	saver.save(sess, '../models/test')
```
14. 自定义命令行参数
	+ 首先定义有哪些参数需要在运行时指定
	+ 程序当中如何获取定义的命令行参数
```
### 定义命令行参数
tf.app.flags.DEFINE_integer('max_step', 100, '模型训练的步数')
tf.app.flags.DEFINE_string('model_dir', '../model/test', '模型保存的路径')

### 获取命令行参数并解析
Flags = tf.flags.FLAGS
Flags.max_step
```
### 数据读取
#### 队列和多线程
1. Tensorflow中的多线程是真正意义上的多线程，并行执行任务
2. 针对读取数据的IO阻塞，Tensorflow使用主线程训练模型，子线程读取数据，通过队列进行数据的传递，高效利用CPU
##### 队列
1. tf.FIFOQueue(capacity,dtypes,name)
	+ capacity  容量，设置队列的大小
	+ dtypes    DType对象列表，队列的元素形状
	+ 主要方法
		1. dequeue(name=None)  出队列
		2. enqueue(val,name=None)  进队列
		3. enqueue_many(vals,name=None)
			* vals 可以是列表或者元组
			* vals的形式必须是 [[xx],]，因为单独的vals==>[xx]会被TensorFlow认为是张量
			* 类比Python中的元组，(1)被认为是1整型，(1,)则被认为是元组
**注意点: 具有依赖性的op只需要执行最后一步**
```
### 模拟同步过程
# -*- coding: utf-8 -*-
# __author__ : Ricky
# __createTime__ : 2019/12/3  22:28 
# __fileName__ : Queue2Thread.py
# __devIDE__ : PyCharm


import tensorflow as tf
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

### 定义队列
Q = tf.FIFOQueue(3, dtypes=tf.float32)

### 初始化队列op
en_op = Q.enqueue_many([[1.0,2.0,3.0],])

### 定义一些处理数据的逻辑，取数据的过程  取队列数据+1  再去入队列
### 以下三步，层层依赖，所以在session中只需要run最后一步的op操作即可
out_q = Q.dequeue()
data = out_q + 1
en_q = Q.enqueue(data)

with tf.Session() as sess:
    ### 初始化队列op
    sess.run(en_op)

    ### 处理数据
    for i in range(100):
        sess.run(en_q)

    ### 训练数据
    for i in range(Q.size().eval()):
        print(sess.run(out_q))

```
2. 队列管理器(创建线程)
	+ tf.train.QueueRunner(queue,enqueue_ops=None)
		1. queue  要操作的队列
		2. enqueue_ops 添加线程要操作的队列op
			* [要操作的队列op-A]*2 表示的就是指定两个线程来执行A
		3. create_threads(sess,coord=None,start=False)
			* 表示真正创建线程来运行指定的队列op操作
			* start  表示在创建时就直接启动线程，为False则需要调用start()来开启线程
			* coord  线程协调器  后面线程管理需要用到
			* return 返回创建的线程实例
3. 线程协调器
	+ tf.train.Coordinator()  协调线程的终止
		1. request_stop()    请求管理的所有线程停止
		2. should_stop()     子线程定期检查停止状态
		3. join(threads=None,stop_grace_period_secs=120,ignore_live_threads=False)
			* threads  线程列表，要求停止的线程
			* stop_grace_period_secs  调用request_stop()之后给线程停止的秒数。默认是2分钟
			* ignore_live_threads  如果为False，则在stop_grace_period_secs限期之后，如果还有所管理的线程存活，则会引发错误
4. CancelledError (see above for traceback): Enqueue operation was cancelled
	+ 出现报错的原因是: 主线程结束，sess关闭，sess是资源管理器，关闭后队列资源就被释放，所以后续的入栈操作就会找不到队列对象
	+ 解决方式
		1. 第一种方法----将线程定义为守护线程。即主线程结束，创建的子线程自动回收
		```
		threads = qr.create_threads(sess,start=True,daemon=True)
		```
		2. 第二种方法----使用线程管理器来管理这些子线程的停止
```
# -*- coding: utf-8 -*-
# __author__ : Ricky
# __createTime__ : 2019/12/3  22:28 
# __fileName__ : Queue2Thread_syc.py
# __devIDE__ : PyCharm


import tensorflow as tf
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
### 模拟异步操作  子线程存入数据  主线程读取数据


# 1. 定义一个队列  1000
Q = tf.FIFOQueue(1000, dtypes=tf.float32)

# 2. 定义要执行的队列op  循环+1 放入队列
var = tf.Variable(0.0)

### 实现变量var的自增  使用tf.assign_add()

data = tf.assign_add(var, tf.constant(1.0))

en_q = Q.enqueue(data)

# 3. 定义队列管理器op，指定多少个子线程应高要操作的op
qr = tf.train.QueueRunner(Q, enqueue_ops=[en_q]*2)
coord = tf.train.Coordinator()
### 初始化变量
init_op = tf.global_variables_initializer()
# 4. 开启会话
with tf.Session() as sess:
    # 运行变量初始化op
    sess.run(init_op)

    ## 创建线程
    threads = qr.create_threads(sess,start=True,coord=coord)

    for i in range(100):
        print(sess.run(Q.dequeue()))

    coord.request_stop()

    coord.join(threads)

```
### 文件读取
1. 文件读取的大致流程
	+ 构造一个文件队列
	+ 读取队列中的内容
	+ 解码内容
2. TensorFlow默认读取规则
	+ 默认只读取一个样本
	+ 针对不同的样本类型
		+ CSV  读取一行，解码
		+ 二进制文件     指定一个样本的Bytes读取
		+ 图片文件       读取一张
3. 批量读取样本
	+ 样本队列

#### CSV文件读取案例
1. 文件读取API
	+ 文件队列构造---tf.train.string_input_producer(string_tensor,shuffle=True)
		1. string_tensor  含有文件名的1阶张量
		2. num_epochs     默认无限过数据
		3. return  具有输出字符串的队列
	+ 文件阅读器
		1. tf.TextLineReader   阅读文本文件逗号分隔值CSV格式文件，默认按行读取
		2. tf.FixedLengthRecordReader(record_bytes)
			* 读取二进制文件
			* record_bytes  指定每次读取的字节数(即单个样本的字节大小)
		3. tf.TFRecordReader  读取TfRecords文件
		4. 共同的读取方法
			* read(file_queue)从队列中指定数量的内容，返回的是Tensor类型的元组，key-value
	+ 文件内容解码器----由于从文件中读取的是字符串，需要函数去解析这些字符串到张量
		1. tf.decode_csv(records,record_default=None,field_delim=None,name=Noe)
			* 将CSV转换为张量，与tf.TextLineReader()搭配使用
			* field_delim 字段分隔符  默认是逗号
			* records  Tensor类型的字符串，每个字符串是csv中的记录行
			* record_defaults 指定读取的每一列数据的类型(每一行记录中字段的数据类型)以及默认值
				1. [[1],[2]]  表示的是两列。每一列数据的类型都是整型且默认值为1
				2. [['None'],[1.0]] 第一列为字符串默认为None，第二列为浮点型默认值为1.0
		2. tf.decode_raw(bytes,out_type,little_endian=None,name=None)
			* 将字节转换为一个数字向量
			* 二进制读取格式默认为UTF-8
	+ 开启读文件的线程
		1. threads = tf.train.start_queue_runners(sess,coord)
			* sess 当前会话
			* coord 线程协调器
	+ 管道批处理
		1. tf.train.batch(tensors,batch_size,num_threads=1,capacity=32,name=None)
			* 读取指定大小的张量
			* tensors  包含张量的列表(单个样本的字段列表)
			* batch_size 从队列中读取张量的批处理大小
				1. 直接影响当前批次读取的样本数量，与capacity和数据大小无关
				2. 当batch_size>总记录行数时，会循环读取数据(直至读取数据的数量等于当前batch_size)
			* num_threads  进入队列的线程数
			* capacity  整数，队列的容量
			* return  批处理读取的张量
		2. tf.train.shuffle_batch(tensors,batch_size,num_threads=1,min_after_dequeuecapacity=32,name=N
			* 打乱队列后读取
			* 同上
			* min_after_dequeue  留下队列里的张量个数，能够保持随机洗牌(随机打乱)

```
# -*- coding: utf-8 -*-
# __author__ : Ricky
# __createTime__ : 2019/12/14  21:12 
# __fileName__ : rcsvReader.py
# __devIDE__ : PyCharm

import tensorflow as tf
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'


def csvReader(fileList):
    """
    CSV文件读取案例
    :param fileList:
    :return:
    """
    ### 1. 构造文件队列
    file_queue = tf.train.string_input_producer(fileList)

    ### 2. 构造csv文件阅读器读取队列数据(一次读取一行)
    reader = tf.TextLineReader()
    key, value = reader.read(file_queue)
    print(key, value)
    ### 3. 对数据进行解码
    records = [['None'],['None']]
    x,y = tf.decode_csv(value, record_defaults=records)

    ### 4. 批处理读取多个数据样本
    x_train, y_train = tf.train.batch([x,y], num_threads=1, capacity=32, batch_size=9)

    return x_train, y_train



if __name__ == '__main__':
    fileNames = os.listdir('./data/csvdata')
    fileList = [os.path.join('./data/csvdata/', file) for file in fileNames]
    print(fileList)
    x_train, y_train = csvReader(fileList)
    with tf.Session() as sess:
        ### 定义一个线程协调器
        coord = tf.train.Coordinator()

        ### 开启读文件线程
        threads = tf.train.start_queue_runners(sess, coord)

        print(sess.run([x_train, y_train]))




```
### 图像读取
1. 图像数字化三要素
	+ 长度
	+ 宽度
	+ 通道数
		* 单通道   灰度值(0~255)   每一个像素点的值都是255以内
		* 三通道   RGB            每一个像素点的值都是RGB三个类别的值的合成，即RGB对应的三张灰度图的合成
2. 训练数据的原则
	+ 需要保证每一个样本的特征值相同(图像的特征其实就是像素值)
3. 图片的基本操作
	+ 缩放图片
4. API
	+ 缩小图片----tf.image.resize_images(images,size)
		1. images 
			* 4-D 形状   [batch,height,width,channels]
			* 3-D 形状   [ height,width,channels]
		2. size  1-D int32张量 new_height,new_width  图像的新尺寸
		3. 返回4-D格式或者3-D格式的图片
	+ tf.WholeFileReader()
		1. 将文件的全部内容作为值输出
		2. return  读取器实例
		3. read(file_queue) 输出为key-value形式，即文件名--文件内容
	+ tf.image.decode_jpeg(contents)
		1. 将JPEG编码的图像解码为uint8张量
		2. return  uint8张量，3-D形状 [height,width.channels]
	+ tf.image.decode_png(contents)
		1. 将PNG编码的图像解码为uint8或者uint16张量
		2. return 张量类型，3-D形状
5. 图片批处理案例流程
	+ 构造图片文件队列
	+ 构造图片阅读器
	+ 读取图片数据
	+ 处理图片数据(其中多一步----统一图片的大小)----保证样本的特征值数量相同，即像素值相同
6. 处理图片的大小(统一大小)
```
image_resize = tf.image.resize_images(image,[200,200])
### 一定要把样本的形状完全固定下来  [200,200,3] 因为不固定的话，在批处理时，系统无法得知通道数，也就无法对读取数量的图片进行规整
```
7. 图片存储，计算的类型
	+ 存储        uint8  节约空间
	+ 矩阵计算    float32 提高精度
[二进制图片数据集](http://www.cs.toronto.edu/~kriz/cifar.html)
8. 读取二进制文件(以图片为例 目标值+特征值  特征值为 32*32*3=3072个像素值)
	+ 读取完后需要对目标值和特征值进行分割
	+ tf.slice(input,begin,size)  要一层一层看
```
# -*- coding: utf-8 -*-
# __author__ : Ricky
# __createTime__ : 2019/12/14  21:12 
# __fileName__ : rcsvReader.py
# __devIDE__ : PyCharm

import tensorflow as tf
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'


class binReader:

    def __init__(self, fileList):
        self.fileList = fileList
        self.height = 32
        self.width = 32
        self.channels = 3
        self.label_bytes = 1
        self.image_bytes = self.height*self.width*self.channels
        self.bytes = self.label_bytes + self.image_bytes

    def read_and_decode(self):
        ### 1. 构造文件队列
        file_queue = tf.train.string_input_producer(self.fileList)

        ### 2. 构造csv文件阅读器读取队列数据(一次读取一行)
        reader = tf.FixedLengthRecordReader(self.bytes)
        key, value = reader.read(file_queue)
        ### 3. 对数据进行解码
        ## 得到的是  目标值+特征值的一维列表
        label_image = tf.decode_raw(value, tf.uint8)
        label = tf.cast(tf.slice(label_image, [0], [self.label_bytes]),tf.int32)
        image = tf.slice(label_image, [self.label_bytes], [self.image_bytes])
        ### 4. 对图片数据进行大小规范
        image_reshape = tf.reshape(image, [self.height, self.width, self.channels])
        print(image_reshape)
        ### 5. 批处理读取多个数据样本
        image_batch, label_batch = tf.train.batch([image_reshape,label], batch_size=10,capacity=10, num_threads=1)
        return image_batch, label_batch


if __name__ == '__main__':
    fileNames = os.listdir('./data/cifar-10')
    fileList = [os.path.join('./data/cifar-10/', file) for file in fileNames]
    print(fileList)

    binObj = binReader(fileList)
    image_batch, label_batch = binObj.read_and_decode()

    with tf.Session() as sess:
        ### 定义一个线程协调器
        coord = tf.train.Coordinator()

        ### 开启读文件线程
        threads = tf.train.start_queue_runners(sess, coord)

        print(sess.run([image_batch, label_batch]))
```
9. TFRecords分析、存取
	+ TFRecords是TensorFlow设计的一种内置的文件格式，也是一种二进制文件
		1. 可以更好地利用内存
		2. 更方便复制和移动
	+ 作用: 为了将二进制数据和标签(训练的类别标签)数据存储在同一个文件中
	+ 文件格式: .tfrecords
	+ 写入文件内容: Example协议块(类字典形式)
	+ 存储
		1. 建立TFRecord存储器
			* tf.python_io.TFRecordWriter(path)
				1. path  要写入records的文件路径
				2. return  文件句柄
				3. method
					+ write(record) 向文件写入一个Example 
						1. 写入时需要对example进行序列化 example.SerializeToString()
					+ close()   关闭文件
	+ 构造每一个样本的Example协议块
		1. tf.train.Example(features=None)
			* 写入tfrecords文件
			* features: tf.train.Features类型的特征实例
			* return  example格式的协议块
		2. tf.train.Features(feature=None)
			* 构建每一个样本的信息键值对
			* feature: 字典数据，key为要保存的名字，value为tf.train.Feature实例
			* return  Features类型
		3. tf.train.Feature(**option)
			* **options
				1. bytes_list = tf.train.BytesList(value=[Bytes])
				2. int64_list = tf.train.Int64List(value=[Value])
				3. floatList = tf.train.FloatList(value=[value])
	+ 读取
		1. 同文件阅读器流程，中间需要解析example协议块
		2. tf.parse_single_example(serialized,features=None,name=None)
			* 解析单个Example协议块
			* serialized  标量字符串Tensor，就是上面存储时序列化的Example
			* features  dict字典数据，键为读取的名字，值为FixedLengthFeature
			* return 一个键值对组成的字典
		3. tf.FixedLenFeature(shape,dtype)
			* shape   输入数据的形状  一般不指定，为空列表
			* dtype   输入数据的类型，与存进文件的类型要一致
				1. int64
				2. float32
				3. string
```
# -*- coding: utf-8 -*-
# __author__ : Ricky
# __createTime__ : 2019/12/14  21:12 
# __fileName__ : rcsvReader.py
# __devIDE__ : PyCharm

import tensorflow as tf
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'


class binReader:

    def __init__(self, fileList):
        self.fileList = fileList
        self.height = 32
        self.width = 32
        self.channels = 3
        self.label_bytes = 1
        self.image_bytes = self.height*self.width*self.channels
        self.bytes = self.label_bytes + self.image_bytes

    def read_and_decode(self):
        ### 1. 构造文件队列
        file_queue = tf.train.string_input_producer(self.fileList)

        ### 2. 构造csv文件阅读器读取队列数据(一次读取一行)
        reader = tf.FixedLengthRecordReader(self.bytes)
        key, value = reader.read(file_queue)
        ### 3. 对数据进行解码
        ## 得到的是  目标值+特征值的一维列表
        label_image = tf.decode_raw(value, tf.uint8)
        label = tf.cast(tf.slice(label_image, [0], [self.label_bytes]),tf.int32)
        image = tf.slice(label_image, [self.label_bytes], [self.image_bytes])
        ### 4. 对图片数据进行大小规范
        image_reshape = tf.reshape(image, [self.height, self.width, self.channels])
        print(image_reshape)
        ### 5. 批处理读取多个数据样本
        image_batch, label_batch = tf.train.batch([image_reshape,label], batch_size=10,capacity=10, num_threads=1)
        return image_batch, label_batch

    def write_to_tfrecords(self, image_batch, label_batch):
        writer = tf.python_io.TFRecordWriter('./data/cifar-10/test.tfrecords')
        for i in range(10):
            image = image_batch[i].eval().tostring()
            label = int(label_batch[i].eval()[0])
            example = tf.train.Example(features=tf.train.Features(feature={
                "image": tf.train.Feature(bytes_list=tf.train.BytesList(value=[image])),
                "label": tf.train.Feature(int64_list=tf.train.Int64List(value=[label]))
            }))

            ### 建立Example存储器

            writer.write(example.SerializeToString())
        writer.close()

    def read_from_tfrecords(self, path):
        ### 1. 构造文件队列
        file_queue = tf.train.string_input_producer([path])

        ### 2. 构造csv文件阅读器读取队列数据(一次读取一行)
        reader = tf.TFRecordReader()
        key, value = reader.read(file_queue)
        ### 3. 对Example协议块进行解析
        features = tf.parse_single_example(value, features={
            "image": tf.FixedLenFeature([],tf.string),
            "label": tf.FixedLenFeature([],tf.int64)
        })
        ### 4. 对数据进行解码

        label = tf.cast(features['label'], tf.int32)
        image = tf.decode_raw(features['image'], tf.uint8)

        image_reshape = tf.reshape(image, [self.height, self.width, self.channels])

        ### 5. 批处理读取多个数据样本
        image_batch, label_batch = tf.train.batch([image_reshape, label], batch_size=10, capacity=10, num_threads=1)
        return image_batch, label_batch


if __name__ == '__main__':
    fileNames = os.listdir('./data/cifar-10')
    fileList = [os.path.join('./data/cifar-10/', file) for file in fileNames if '_' in file]
    print(fileList)

    binObj = binReader(fileList)
    # image_batch, label_batch = binObj.read_and_decode()
    image_batch, label_batch = binObj.read_from_tfrecords('./data/cifar-10/test.tfrecords')

    with tf.Session() as sess:
        ### 定义一个线程协调器
        coord = tf.train.Coordinator()

        ### 开启读文件线程
        threads = tf.train.start_queue_runners(sess, coord)
        # binObj.write_to_tfrecords(image_batch, label_batch)

        print(sess.run([image_batch, label_batch]))
        coord.request_stop()
        coord.join(threads)
```
## 神经网络
### 感知机
1. 定义: 有n个输入数据，通过权重与各数据之间的计算和，比较激活函数的结果，得出输出
2. 应用: 很容易解决与、或问题----解决分类问题
	+ 当一个感知机无法完成指定目标的分类时，可以增加感知机
[谷歌在线神经网络实验](http://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.76485&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false)
3. 逻辑回归与感知机的联系
	+ 感知机是通过简单地设置阈值来达到分类的效果的
	+ 逻辑回归则是在线性回归的基础上使用了sigmoid映射函数----在感知机中被称为是激活函数

### 神经网络
1. 定义: 在机器学习和认知科学领域，人工神经网络(artificial neural network，缩写为ANN),简称为神经网络(缩写为NN)
	+ 一种模仿生物神经网络的结构和功能的计算模型
	+ 用于对函数进行估计或近似
2. 神经网络的种类
	+ 基础神经网络
		1. 单层感知器
		2. 线性神经网络
		3. BP神经网络
		4. Hopfield神经网络
		5. ...
	+ 进阶神经网络
		1. 玻尔兹曼机 
		2. 受限玻尔兹曼机
		3. 递归神经网络
		4. ...
	+ 深度神经网络
		1. 深度置信网络
		2. 卷积神经网络
		3. 循环神经网络
		4. LSTM网络
		5. ...
3. 感知机(神经元) ---> 多个 ---> 神经网络()
	+ 神经网络在某些方面是无法解释的  (比如: 有些算法的分类过程是无法解释的)
4. 神经网络的特点
	+ 输入向量的维度和输入神经元的个数相同
	+ 每一个连接都有权重值
	+ 同一层神经元之间没有连接
	+ 由输入层、隐层、输出层组成
		1. 隐层   模糊不清、无法解释的转换过程
	+ 第N层与第N-1层的所有神经元连接，也叫全连接(一般全连接层都是需要的)
5. 神经网络的组成
	+ 结构: 神经网络中的权重，神经元等等
	+ 激活函数
	+ 学习规则
		1. 学习规则指定了网络中的权重如何随着时间推进而调整(反向传播算法)

6. 神经网络解决多分类问题
	+ 某一个样本作为输入，输出是该样本属于全部类别的每一个概率
	+ 根据哪个类别的概率最大来得出该样本最终的类别

7. tensorFlow中神经网络API模块
	+ tf.nn    
		1. 提供神经网络相关操作的支持
		2. 包括卷积操作(conv)
		3. 池化操作(pooling)
		4. 归一化
		5. loss
		6. 分类操作
		7. embedding
		8. RNN
		9. Evaluation
	+ tf.layers
		1. 主要提供高层的神经网络，主要和卷积有关。对tf.nn的进一步封装
	+ tf.contrib
		1. tf.contrib.layers提供能够将计算图中的网络层、正则化、摘要操作
		2. 是构建计算图的高级操作

8. softmax回归----解决多分类问题
	+ 特征输入---->经过一层变换后--->得出输出层结果
	+ 对结果值进行softmax算法转换为概率值
		1. 概率值相加等于1
		2. 算法公式: y=e的当前结果次方除以每一个结果的e的输出结果次方

9. 算法的比较
	+ 算法            策略       优化
	+ 线性回归        均方误差    梯度下降
	+ 逻辑回归        对数似然    梯度下降
	+ softmax回归     交叉熵损失  反向传播算法(就是梯度下降)

10. 交叉熵损失
	+ 样本的类别使用one-hot编码表示
	+ 一个样本就有一个交叉熵损失
	+ 交叉熵就是经过softmax算法后的结果值与真实值之间的误差
		1. 算法公式: H(y) = -y'log(yi)
			* y' 为真实值
			* yi 为softmax后的结果值  当该值为1时，最终的结果为0，即误差为0，真实值=预测值
11. 传播概念
	+ 正向传播: 就是从输入开始经过一层一层的转换得出输出
	+ 反向传播: 就是根据损失函数，利用梯度下降来一步一步更新每一层的权重值

#### 手写数字识别实现流程
1. 常用API
	+ tf.matmul(a,b,name=None)+bias
		1. return 全连接结果，供交叉损失运算
		2. 不需要激活函数(因为是最后的输出)
	+ tf.nn.softmax_cross_entropy_with_logits(labels=None,logits=None,name=None)
		1. 计算logits和labels之间的交叉熵损失
		2. labels  标签值(真实值)
		3. logits  样本加权后的值
		4. return  返回损失值列表
	+ tf.reduce_mean(input_tensor)
		1. 计算张量的尺寸的元素平均值
	+ tf.train.GradientDescentOptimizer(learning_rate)
		1. learning_rate  学习率0.01
		2. minimize(loss) 最小化损失
		3. return  梯度下降op
	+ tf.one_hot(indices,depth)
		1. indices  在one-hot编码中的位置，即数据集标签
		2. depth    张量的深度，即类别数
2. 获取数据
[数据集链接](http://yann.lecun.com/exdb/mnist/)
```
from tensorflow.examples.tutorials.mnist import  input_data
mnist = input_data.read_data_sets()

```
3. 实例---识别手写数字
```
# -*- coding: utf-8 -*-
# __author__ : Ricky
# __createTime__ : 2019/12/16  0:38 
# __fileName__ : mnist_sort.py
# __devIDE__ : PyCharm


import tensorflow as tf
import os
import numpy as np
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

from tensorflow.examples.tutorials.mnist import  input_data

FLAGS = tf.app.flags.FLAGS

tf.app.flags.DEFINE_string('is_train', 1, "是否训练数据")

def full_connect():
    mnist = input_data.read_data_sets('./data/mnist/', one_hot=True)



    # 1. 建立数据的占位符x  [None,784] y_true [None,10]
    with tf.variable_scope("data"):
        x = tf.placeholder(tf.float32, [None, 784])
        y_true = tf.placeholder(tf.int32, [None,10])

    # 2. 建立一个全连接层的神经网络  w [784,10] b [10]
    with tf.variable_scope("fc_model"):
        # 随机初始化权重和偏置
        weight = tf.Variable(tf.random_normal([784,10], name='w'))
        bias = tf.Variable(tf.constant(0.0,shape=[10]), name='b')

        ## 预测None个样本的输出结果matrix [None,784]*[784,10]
        y_predict = tf.matmul(x,weight)+bias
    # 3. 求出所有样本的损失，然后求出平均值
    with tf.variable_scope("soft_cross"):
        # 求平均交叉熵损失
        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_true, logits=y_predict))
    # 4. 梯度下降求出损失
    with tf.variable_scope("optimizer"):
        train_op = tf.train.GradientDescentOptimizer(0.1).minimize(loss)

    # 5. 计算准确率
    with tf.variable_scope("acc"):
        equal_list = tf.equal(tf.argmax(y_true,1), tf.argmax(y_predict,1))

        ## equal_list None个样本 [1,1,1,1,1,0,0,0,,1,1,1,...]
        accuracy = tf.reduce_mean(tf.cast(equal_list, tf.float32))

    ### 收集变量  单个数字值收集
    tf.summary.scalar("losses", loss)
    tf.summary.scalar("acc", accuracy)

    ### 高纬度变量收集
    tf.summary.histogram("weightes", weight)
    tf.summary.histogram("biases", bias)


    ### 定义变量初始化op
    init_op = tf.global_variables_initializer()

    ### 合并变量
    merged = tf.summary.merge_all()
    ### 实例化保存模型实例
    saver = tf.train.Saver()
    with tf.Session() as sess:
        ## 初始化变量op
        sess.run(init_op)


        ### 建立events事件文件写入句柄
        fileWriter = tf.summary.FileWriter('../logs/', graph=sess.graph)

        if FLAGS.is_train == 1:

            ### 迭代步数去训练，更新参数预测
            for i in range(2000):
                ### 取出真实存在的特征值和目标值(批量取出50个样本)
                x_train,y_train = mnist.train.next_batch(50)
                ### 运行train_op训练数据
                sess.run(train_op, feed_dict={x:x_train, y_true: y_train})

                ### 写入每步训练的值
                summary = sess.run(merged, feed_dict={x:x_train, y_true: y_train})
                fileWriter.add_summary(summary,i)

                print("训练第%d步, 准确率为%f" % (i, sess.run(accuracy, feed_dict={x:x_train, y_true: y_train})))
            ### 保存模型
            saver.save(sess, '../models/fc_model')
        else:
            ### 加载模型
            saver.restore(sess, '../models/fc_model')
            accList = []
            for i in range(100):
                x_test, y_test = mnist.train.next_batch(1)
                acc = sess.run(accuracy, feed_dict={x: x_test, y_true: y_test})
                accList.append(acc)
                print("第%d张图片，手写数字目标值为%d，预测值为%d" %(
                    i,
                    tf.argmax(y_test,1).eval(),
                    tf.argmax(sess.run(y_predict,feed_dict={x: x_test, y_true: y_test}),1).eval()
                ))
            print("当前预测准确率为%f" % np.mean(accList))




    return None


if __name__ == '__main__':
    full_connect()

```
## 深度神经网络
1. 全连接神经网络的缺点
	+ 参数太多，将图片的每一个像素值作为特征值输入，导致权重参数很多，浪费
	+ 没有利用图像之间的位置信息，对于图像识别任务来说，每一个像素与周围的像素都是联系比较紧密的
	+ 层数限制(只有一层)

### 卷积神经网络
1. 特点: 在于将神经网络中的隐藏层分为卷积层和池化层(pooling layer，又称下采样层)
	+ 卷积层: 通过在原始图像上平移来提取特征
	+ 池化层: 通过特征后稀疏参数来减少学习的参数，降低网络的复杂度(最大池化和平均池化)

2. 卷积神经网络的结构
	+ 卷积层过滤器
		1. 个数  
		2. 大小(奇数 1*1 3*3 5*5 ...)
		3. 步长
		4. 零填充
		5. 卷积层输出深度、输出宽度
			* 深度由过滤器个数决定
			* 输出宽度由卷积核大小决定
		6. 输入体积大小  H1*W1*D1
		7. 四个超参数
			* Filter 数量K
			* Filter 大小F
			* 步长 S
			* 零填充大小P
		8. 输出体积大小  H2*W2*D2
			* H2=(H1-F+2P)/S+1
			* W2=(W1-F+2P)/S+1
			* D2=K
	+ 激活函数
	+ 池化层
	+ 全连接层
	+ ....在大型网络当中会有一个droupout层(减少过拟合)

3. 简单概述卷积神经网络
	+ 定义过滤器(观察窗口)大小、步长(移动的像素数量，一般为1)
	+ 移动越过图片大小时的处理方式
		1. 不越过，此时就没有观察完全，停止观察
		2. 直接超过，使用0填充
4. 卷积层的零填充
	+ 卷积核在提取特征映射时的动作称之为padding(零填充)，由于移动步长不一定能整出整张图的像素宽度，此时的处理方式为
		1. SAME   越过边缘取样，取样的面积和输入图像的宽度一致
		2. VALID  不越过边缘取样，取样的面积小于输入图像的像素宽度

5. 卷积网络API
	+ tf.nn.conv2d(input,filter,strides=,padding=,name=None)  卷积操作
		1. 计算给定4-D input和filter张量的2维卷积
		2. input 给定的输入张量，具有[batch,height,width,channel],类型为float32、64
			* batch  指的是几张图片
		3. filter  指定过滤器的大小 [filter_height,filter_width,in_channels,out_channels]
			* in_channels指的是输入通道，图片的颜色通道
			* out_channels指的是输出通道，一共有多少个filter
		4. strides  strides=[1,stride,stride,1],步长
		5. padding  SAME或者VALID
			* SAME使用的是填充，变化后的height，width一样大
			* VALID 舍弃边缘
	+ tf.nn.relu(features,name=None)                激活函数
		1. features  卷积后加上偏置的结果
		2. return 返回结果
	+ tf.nn.max_pool(value,ksize=,strides=,padding=name=None)
		1. value  4-D Tensor形状[batch,height,width,channels]
		2. ksize  池化窗口大小  [1,ksize,ksize,1]
		3. strides 步长大小     [1,strides,strides,1]
		4. padding 同上
6. 激活函数
	+ sigmoid   1/(1+e的-z次方)
		1. 不使用sigmoid函数的原因
			* 反向传播求误差梯度时，计算量相对较大，而采用Relu激活函数，整个过程的计算量可以节省很多
			* 对于深层网络，sogmoid函数反向传播时，很容易就会出现梯度爆炸的情况
	+ relu   max(0,u)  小于0取0，大于0取自身
	+ 作用: 增加网络的非线性分割能力
7. 梯度爆炸的原因
	+ 学习率太高
	+ 激活函数使用不恰当
8. 池化层(Pooling)计算
	+ Pooling层主要的作用是特征提取，通过去掉Feature Map中不重要的样本，进一步减少参数的数量
	+ Pooling的方法很多，最常用的是Max Pooling
		1. 观察窗口2*2  步长2
		2. 取窗口映射范围内的最大值作为当前窗口中所有特征的代表
		3. 结果就会将原先的4*4 稀疏为 2*2

9. 全连接层
	+ 前面的卷积和池化相当于是在做特征工程，后面的全连接相当于在做特征加权
	+ 最后的全连接层在整个卷积神经网络中起到分类器的作用

10. 使用卷积
	+ 当遇到相对复杂的特征时，可以不自定义卷积核去处理，而是	使用一些成熟的框架，如googlenet
	```
	### googlenet
	from tensorflow.contrib.slim.python.slim.nets.inception_v3 import inception_v3_base
	```
	+ 当遇到相对简单的特征时，我们可以自定义卷积核去处理
11. 基于卷积神经网络的手写数字识别
```
# -*- coding: utf-8 -*-
# __author__ : Ricky
# __createTime__ : 2019/12/16  0:38 
# __fileName__ : mnist_sort.py
# __devIDE__ : PyCharm


import tensorflow as tf
import os
import numpy as np
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

from tensorflow.examples.tutorials.mnist import  input_data


FLAGS = tf.app.flags.FLAGS

tf.app.flags.DEFINE_string('is_train', 1, "是否训练数据")

def full_connect():
    mnist = input_data.read_data_sets('./data/mnist/', one_hot=True)



    # 1. 建立数据的占位符x  [None,784] y_true [None,10]
    with tf.variable_scope("data"):
        x = tf.placeholder(tf.float32, [None, 784])
        y_true = tf.placeholder(tf.int32, [None,10])

    # 2. 建立一个全连接层的神经网络  w [784,10] b [10]
    with tf.variable_scope("fc_model"):
        # 随机初始化权重和偏置
        weight = tf.Variable(tf.random_normal([784,10], name='w'))
        bias = tf.Variable(tf.constant(0.0,shape=[10]), name='b')

        ## 预测None个样本的输出结果matrix [None,784]*[784,10]
        y_predict = tf.matmul(x,weight)+bias
    # 3. 求出所有样本的损失，然后求出平均值
    with tf.variable_scope("soft_cross"):
        # 求平均交叉熵损失
        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_true, logits=y_predict))
    # 4. 梯度下降求出损失
    with tf.variable_scope("optimizer"):
        train_op = tf.train.GradientDescentOptimizer(0.1).minimize(loss)

    # 5. 计算准确率
    with tf.variable_scope("acc"):
        equal_list = tf.equal(tf.argmax(y_true,1), tf.argmax(y_predict,1))

        ## equal_list None个样本 [1,1,1,1,1,0,0,0,,1,1,1,...]
        accuracy = tf.reduce_mean(tf.cast(equal_list, tf.float32))

    ### 收集变量  单个数字值收集
    tf.summary.scalar("losses", loss)
    tf.summary.scalar("acc", accuracy)

    ### 高纬度变量收集
    tf.summary.histogram("weightes", weight)
    tf.summary.histogram("biases", bias)


    ### 定义变量初始化op
    init_op = tf.global_variables_initializer()

    ### 合并变量
    merged = tf.summary.merge_all()
    ### 实例化保存模型实例
    saver = tf.train.Saver()
    with tf.Session() as sess:
        ## 初始化变量op
        sess.run(init_op)


        ### 建立events事件文件写入句柄
        fileWriter = tf.summary.FileWriter('../logs/', graph=sess.graph)

        if FLAGS.is_train == 1:

            ### 迭代步数去训练，更新参数预测
            for i in range(2000):
                ### 取出真实存在的特征值和目标值(批量取出50个样本)
                x_train,y_train = mnist.train.next_batch(50)
                ### 运行train_op训练数据
                sess.run(train_op, feed_dict={x:x_train, y_true: y_train})

                ### 写入每步训练的值
                summary = sess.run(merged, feed_dict={x:x_train, y_true: y_train})
                fileWriter.add_summary(summary,i)

                print("训练第%d步, 准确率为%f" % (i, sess.run(accuracy, feed_dict={x:x_train, y_true: y_train})))
            ### 保存模型
            saver.save(sess, '../models/fc_model')
        else:
            ### 加载模型
            saver.restore(sess, '../models/fc_model')
            accList = []
            for i in range(100):
                x_test, y_test = mnist.train.next_batch(1)
                acc = sess.run(accuracy, feed_dict={x: x_test, y_true: y_test})
                accList.append(acc)
                print("第%d张图片，手写数字目标值为%d，预测值为%d" %(
                    i,
                    tf.argmax(y_test,1).eval(),
                    tf.argmax(sess.run(y_predict,feed_dict={x: x_test, y_true: y_test}),1).eval()
                ))
            print("当前预测准确率为%f" % np.mean(accList))




    return None

def init_weight(shape):
    return tf.Variable(tf.random_normal(shape=shape))

def init_bias(shape):
    return tf.Variable(tf.constant(0.0,dtype=tf.float32, shape=shape))

def model():
    # 1. 建立数据的占位符x  [None,784] y_true [None,10]
    with tf.variable_scope("data"):
        x = tf.placeholder(tf.float32, [None, 784])
        y_true = tf.placeholder(tf.int32, [None, 10])

    # 2. 一卷积层  卷积5*5*1,32个 strides=1      激活   池化
    with tf.variable_scope("conv1"):
        # 随机初始化权重和偏置
        conv1_weight = init_weight(shape=[5,5,1,32])
        conv1_bias = init_bias(shape=[32])
        x_reshape = tf.reshape(x, [-1, 28, 28, 1])

        ## relu激活函数
        x_relu1 = tf.nn.relu(tf.nn.conv2d(x_reshape, conv1_weight,strides=[1,1,1,1], padding='SAME') + conv1_bias)
        ### 池化 2*2 strides=2 [None,28,28,32]----> [None,14,14,32]
        x_pool1 = tf.nn.max_pool(x_relu1, ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')

    # 3. 二卷积层  卷积5*5*1,64个 strides=1      激活   池化
    with tf.variable_scope("conv2"):
        # 随机初始化权重和偏置
        conv2_weight = init_weight(shape=[5, 5, 32, 64])
        conv2_bias = init_bias(shape=[64])


        ## relu激活函数
        x_relu2 = tf.nn.relu(tf.nn.conv2d(x_pool1, conv2_weight, strides=[1, 1, 1, 1], padding='SAME') + conv2_bias)
        ### 池化 2*2 strides=2 [None,14,14,32]---> [None,7,7,64]
        x_pool2 = tf.nn.max_pool(x_relu2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')

    # 4. 建立一个全连接层的神经网络  w [7*7*64,10] b [10]
    with tf.variable_scope("fc_model"):
        # 随机初始化权重和偏置
        weight = init_weight(shape=[7*7*64,10])
        bias = init_bias(shape=[10])
        ### 修改形状 [None,7,7,64]   ----> [None,7*7*64]
        fc_reshape = tf.reshape(x_pool2, [-1,7*7*64])
        ## 预测None个样本的输出结果matrix [None,784]*[784,10]
        y_predict = tf.matmul(fc_reshape, weight) + bias
        return x,y_true,y_predict



def conv_fc():
    mnist = input_data.read_data_sets('./data/mnist/', one_hot=True)

    x, y_true, y_predict = model()

    # 1. 求出所有样本的损失，然后求出平均值
    with tf.variable_scope("soft_cross"):
        # 求平均交叉熵损失
        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_true, logits=y_predict))
    # 2. 梯度下降求出损失    深度神经网络中的学习率不能像之前单全连接层那样大  一般都是0.00001~0.01之间不等
    with tf.variable_scope("optimizer"):
        train_op = tf.train.GradientDescentOptimizer(0.01).minimize(loss)

    # 3. 计算准确率
    with tf.variable_scope("acc"):
        equal_list = tf.equal(tf.argmax(y_true, 1), tf.argmax(y_predict, 1))

        ## equal_list None个样本 [1,1,1,1,1,0,0,0,,1,1,1,...]
        accuracy = tf.reduce_mean(tf.cast(equal_list, tf.float32))

    ### 定义变量初始化op
    init_op = tf.global_variables_initializer()

    with tf.Session() as sess:
        ## 初始化变量op
        sess.run(init_op)

        ### 迭代步数去训练，更新参数预测
        for i in range(2000):
            ### 取出真实存在的特征值和目标值(批量取出50个样本)
            x_train, y_train = mnist.train.next_batch(50)
            ### 运行train_op训练数据
            sess.run(train_op, feed_dict={x: x_train, y_true: y_train})



            print("训练第%d步, 准确率为%f" % (i, sess.run(accuracy, feed_dict={x: x_train, y_true: y_train})))


if __name__ == '__main__':
    # full_connect()
    conv_fc()


```
**深度神经网络中的学习率不能像之前单全连接层那样大  一般都是0.00001~0.01之间不等**

## 实战----验证码识别
```
# -*- coding: utf-8 -*-
# __author__ : Ricky
# __createTime__ : 2019/12/20  0:55 
# __fileName__ : captcha_train.py
# __devIDE__ : PyCharm

import tensorflow as tf
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
tf.app.flags.DEFINE_string("captcha_dir", './tfrecords/captcha.tfrecords' , '验证码数据的路径')
tf.app.flags.DEFINE_integer("batch_size", 100 , '每批次要训练的样本数')
tf.app.flags.DEFINE_integer('is_train', 1, "是否训练数据")
tf.app.flags.DEFINE_integer('letter_num', 26, "真实类别的个数")
tf.app.flags.DEFINE_integer('label_num', 4, "每个样本的目标值个数")


FLAGS = tf.app.flags.FLAGS



def init_weight(shape):
    return tf.Variable(tf.random_normal(shape=shape))

def init_bias(shape):
    return tf.Variable(tf.constant(0.0,dtype=tf.float32, shape=shape))





def fc_model(image_batch):
    """
    进行预测结果
    :param image_batch: 100 图片特征值
    :return: y_predict的预测值 [100, 4*26]
    """
    with tf.variable_scope("fc_model"):
        # 随机初始化权重和偏置
        weight = init_weight(shape=[20 * 80 * 3, 4 * 26])
        bias = init_bias(shape=[4 * 26])
        ### 修改形状 [None,7,7,64]   ----> [None,7*7*64]
        image_reshape = tf.reshape(image_batch, [-1, 20 * 80 * 3])
        # 进行全连接
        ## 预测None个样本的输出结果matrix [None,784]*[784,10]
        # 计算时需要使用float32
        y_predict = tf.matmul(tf.cast(image_reshape, tf.float32), weight) + bias
        return y_predict

def toOneHot(label):
    """
    将读取文件当中的 目标值转换为one-hot编码
    :param label: [100,4] ---> [[12,25,12,25], [....]]
    :return: one-hot
    """
    # 进行one-hot编码，提供给交叉熵损失计算、准确率计算
    label_oneHot =  tf.one_hot(label,depth=FLAGS.letter_num, axis=2)

    return label_oneHot

def read_and_decode():
    """
    读取验证码数据API
    :return:  image_batch,label_batch
    """
    ### 1. 构造文件队列
    file_queue = tf.train.string_input_producer([FLAGS.captcha_dir])

    ### 2. 构造阅读器，读取文件内容，默认是一个样本
    reader = tf.TFRecordReader()
    key, value = reader.read(file_queue)
    ### 3. tfrecords文件数据需要对example协议块进行解析
    features = tf.parse_single_example(value, features={
        "image": tf.FixedLenFeature([], tf.string),
        "label": tf.FixedLenFeature([], tf.string)
    })

    ### 4. 对features数据进行解码  先解析图片的特征值，再去解析标签值
    image = tf.decode_raw(features['image'], tf.uint8)
    label = tf.decode_raw(features['label'], tf.uint8)
    print(image, label)

    image_reshape = tf.reshape(image, [20,80,3])
    print(image_reshape)

    label_reshape = tf.reshape(label, [4])
    print(label)
    ### 5. 批处理读取多个数据样本
    batch_size = FLAGS.batch_size
    image_batch, label_batch = tf.train.batch([image_reshape, label_reshape], batch_size=batch_size, capacity=batch_size, num_threads=1)
    return image_batch, label_batch

def captcharec():
    """
    验证码识别程序
    :return:
    """
    # 1. 读取验证码数据文件
    image_batch,label_batch = read_and_decode()
    # 2. 通过输入图片特征数据，建立模型，得出预测结果
    # 只操作一层 全连接层
    y_predict = fc_model(image_batch)  # 【100,4*26】
    print(y_predict)
    # 3. 先把目标值转换为one-hot编码
    y_true = toOneHot(label_batch)    # 【100,4,26】

    # 4. 求出所有样本的损失，然后求出平均值
    with tf.variable_scope("soft_cross"):
        # 求平均交叉熵损失
        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf.reshape(y_true, [FLAGS.batch_size, FLAGS.label_num*FLAGS.letter_num]), logits=y_predict))
    # 5. 梯度下降求出损失
    with tf.variable_scope("optimizer"):
        train_op = tf.train.GradientDescentOptimizer(0.01).minimize(loss)

    # 6. 计算准确率
    with tf.variable_scope("acc"):
        equal_list = tf.equal(tf.argmax(y_true, 2), tf.argmax(tf.reshape(y_predict,[FLAGS.batch_size,FLAGS.label_num,FLAGS.letter_num]), 2))

        ## equal_list None个样本 [1,1,1,1,1,0,0,0,,1,1,1,...]
        accuracy = tf.reduce_mean(tf.cast(equal_list, tf.float32))

        ### 定义变量初始化op
    init_op = tf.global_variables_initializer()

    ### 实例化保存模型实例
    saver = tf.train.Saver()

    with tf.Session() as sess:
        ## 初始化变量op
        sess.run(init_op)

        ## 定义线程协调器和开启线程(有数据在文件当中读取提供给模型)
        coord = tf.train.Coordinator()

        ## 开启线程去批量读取文件数据
        threads = tf.train.start_queue_runners(sess=sess, coord=coord)

        if FLAGS.is_train == 1:

            ### 迭代步数去训练，更新参数预测
            for i in range(5000):

                ### 运行train_op训练数据
                sess.run(train_op)

                print("训练第%d步, 准确率为%f" % (i, accuracy.eval()))
            ### 保存模型
            saver.save(sess, './models/fc_model')
        else:
            pass
            ### 加载模型
            # saver.restore(sess, './models/fc_model')
            # accList = []
            # for i in range(100):
            #     x_test, y_test = mnist.train.next_batch(1)
            #     acc = sess.run(accuracy, feed_dict={x: x_test, y_true: y_test})
            #     accList.append(acc)
            #     print("第%d张图片，手写数字目标值为%d，预测值为%d" % (
            #         i,
            #         tf.argmax(y_test, 1).eval(),
            #         tf.argmax(sess.run(y_predict, feed_dict={x: x_test, y_true: y_test}), 1).eval()
            #     ))
            # print("当前预测准确率为%f" % np.mean(accList))
        # 回收线程
        coord.request_stop()
        coord.join(threads)

if __name__ == '__main__':
    captcharec()


```